{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavanvarma007/-Pavan_INFO5731_Spring2023/blob/main/Copy_of_in_class_exercise_05_04182023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6key6w2l0-E"
      },
      "source": [
        "# **The fifth in-class-exercise (40 points in total, 4/18/2023)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnMOkd5Vl0-F"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
        "\n",
        "Algorithms:\n",
        "\n",
        "(1) MultinominalNB\n",
        "\n",
        "(2) SVM \n",
        "\n",
        "(3) KNN \n",
        "\n",
        "(4) Decision tree\n",
        "\n",
        "(5) Random Forest\n",
        "\n",
        "(6) XGBoost\n",
        "\n",
        "(7) Word2Vec\n",
        "\n",
        "(8) BERT\n",
        "\n",
        "Evaluation measurement:\n",
        "\n",
        "(1) Accuracy\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Precison \n",
        "\n",
        "(4) F-1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO99Wqztl0-H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "from xgboost import XGBClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_d_frme = pd.read_csv(r'stsa-train.txt',sep = 'delimiter=',header= None,names=['Text'])\n",
        "train_d_frme[['Sentiment','Text']] = train_d_frme[\"Text\"].str.split(\" \", 1, expand=True)\n",
        "train_d_frme.head()\n"
      ],
      "metadata": {
        "id": "Ft4xTYToHhVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_d_frme = pd.read_csv(r'stsa-test.txt',sep = 'delimiter=',header= None,names=['Text'])\n",
        "test_d_frme[['Sentiment','Text']] = test_d_frme[\"Text\"].str.split(\" \", 1, expand=True)"
      ],
      "metadata": {
        "id": "CygJOOfhDEic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_d_frme.head()\n"
      ],
      "metadata": {
        "id": "2i2sLDirDG2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopword=nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "w_l= WordNetLemmatizer()\n",
        "def cleantext(txt):\n",
        "  txt=\"\".join([w.lower() for w in txt if w not in string.punctuation])\n",
        "  txt = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", txt)\n",
        "  tok = re.split('\\W+',txt)\n",
        "  txt = [w_l.lemmatize(w1) for w1 in tok if w1 not in stopword]\n",
        "  return txt"
      ],
      "metadata": {
        "id": "2kJD9x6mDLpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('omw-1.4')\n",
        "tf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "X_i_df = tf_vect.fit_transform(train_dfrme['Text'])\n",
        "print(X_i_df.shape)\n",
        "X_i_df_dafrme=pd.DataFrame(X_idf.toarray())\n",
        "X_i_df_dafrme.columns=tf_vect.get_feature_names()\n",
        "X_test_idf = tf_vect.transform(test_dfrme['Text'])\n",
        "print(X_idf.shape)"
      ],
      "metadata": {
        "id": "pPKixgaeDOcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNB = MultinomialNB()\n",
        "SVM = LinearSVC()\n",
        "KNN = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n",
        "DT = DecisionTreeClassifier()\n",
        "RF = RandomForestClassifier()\n",
        "XGB = XGBClassifier()\n",
        "xtrain, xtest, ytrain, ytest = traintestsplit(X_idf_dafrme, train_dfrme['Sentiment'].values,\n",
        "                                                test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "M9J0Em4cDTdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "MNB_test = MNB.fit(x_train,ytrain)\n",
        "Y_MNB = MNB_test.predict(xtest)\n",
        "print('Accuracy %s' % accuracy_score(Y_MNB,ytest))\n",
        "print(classification_report(ytest,Y_MNB))\n",
        "from sklearn.model_selection import cross_val_score\n",
        "sco_MNB = cross_val_score(MNB, xtest, ytest, cv=10)\n",
        "print(\"Accuracy using MNB\",sco_MNB.mean())"
      ],
      "metadata": {
        "id": "b8lmD9mzDWgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVtest = SVM.fit(xtrain,ytrain)\n",
        "YSVM = SVM_test.predict(xtest)\n",
        "print('Accuracy %s' % accuracy_score(YSVM,ytest))\n",
        "print(classification_report(ytest,YSVM))\n",
        "from sklearn.model_selection import cross_val_score\n",
        "sco_SVM = cross_val_score(SVM, xtest, ytest, cv=10)\n",
        "print(\"Accuracy using SVM\",sco_SVM.mean())"
      ],
      "metadata": {
        "id": "O0exg2E3DZGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KNNtest = KNN.fit(xtrain,ytrain)\n",
        "YKNN = KNNtest.predict(xtest)\n",
        "print('Accuracy %s' % accuracy_score(YKNN,ytest))\n",
        "print(classification_report(ytest,YKNN))\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores_KNN = cross_val_score(KNN, xtest, ytest, cv=10)\n",
        "print(\"Accuracy using knn\",scores_KNN.mean())"
      ],
      "metadata": {
        "id": "KFYxibtmDcHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec_tree_test = DT.fit(x_train,y_train)\n",
        "Y_dec_tree = dec_tree_test.predict(x_test)\n",
        "print('Accuracy %s' % accuracy_score(Y_dec_tree,y_test))\n",
        "print(classification_report(y_test,Y_dec_tree))\n",
        "scores_DT = cross_val_score(DT, x_test, y_test, cv=10)\n",
        "print(\"Accuracy of Decision trees\",scores_DT.mean())\n"
      ],
      "metadata": {
        "id": "62OaLu0cDfll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF_test = RF.fit(x_train,y_train)\n",
        "Y_RF = RF_test.predict(x_test)\n",
        "print('Accuracy %s' % accuracy_score(Y_RF,ytest))\n",
        "print(classification_report(ytest,Y_RF))\n",
        "sco_RF = cross_val_score(RF, xtest, ytest, cv=10)\n",
        "print(\"Accuracy using Random Forest\",sco_RF.mean())"
      ],
      "metadata": {
        "id": "HhAFqpTmDip9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_test = XGB.fit(x_train,y_train)\n",
        "Y_XGB = XGB_test.predict(x_test)\n",
        "print('Accuracy %s' % accuracy_score(Y_XGB,y_test))\n",
        "print(classification_report(y_test,Y_XGB))\n",
        "sco_XGB = cross_val_score(XGB, x_test, y_test, cv=10)\n",
        "print(\"Accuracy using XGBoost\",sco_XGB.mean())"
      ],
      "metadata": {
        "id": "c0Q12IMODlad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy using MNB\",sco_MNB.mean())\n",
        "print(\"Accuracy using SVM\",sco_SVM.mean())\n",
        "print(\"Accuracy using knn\",scores_KNN.mean())\n",
        "print(\"Accuracy using Decision trees\",scores_DT.mean())\n",
        "print(\"Accuracy using Random Forest\",sco_RF.mean())\n",
        "print(\"Accuracy using XGBoost\",sco_XGB.mean())"
      ],
      "metadata": {
        "id": "kSx5yQFFDo_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_MNB = MNB_test.predict(X_test_idf)\n",
        "print('Final trained model(MNB) with high accuracy evaluated based on the test data: %s' % accuracy_score(predict_MNB,test_dfrme['Sentiment']))"
      ],
      "metadata": {
        "id": "AhlNar6JDpvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L_A6oSWiDsoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8px-vb6Kl0-I"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "Apply the listed clustering methods to the dataset:\n",
        "\n",
        "K-means\n",
        "\n",
        "DBSCAN\n",
        "\n",
        "Hierarchical clustering\n",
        "\n",
        "Word2Vec\n",
        "\n",
        "BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below. \n",
        "https://www.kaggle.com/karthik3890/text-clustering "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8Z2tKphl0-I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "sample_data = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
        "sample_data.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(sample_data.Rating)"
      ],
      "metadata": {
        "id": "mVXVBt3HDw90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data(x1):\n",
        "    if x1 < 3:\n",
        "        return 'negative'\n",
        "    return 'positive'\n",
        "act_Sco = sample_data['Rating']\n",
        "posNeg = act_Sco.map(part) \n",
        "sample_data['RatingPosNeg'] = posNeg\n",
        "sns.countplot(sample_data.RatingPosNeg)"
      ],
      "metadata": {
        "id": "uuGG1gx0D3K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "sno = nltk.stem.SnowballStemmer('english') \n",
        "st=set(stopwords.words('english'))\n",
        "\n",
        "sample = sample_data"
      ],
      "metadata": {
        "id": "C7tiAq1ZD-EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleandata(se):\n",
        "    clean_dt = re.sub(r'[?|!|\\'|\"|#]',r'',se)\n",
        "    clean_dt = re.sub(r'[.|,|)|(|\\|/]',r' ',clean_dt)\n",
        "    return  clean_dt\n",
        "result = sample_data.sample(n=10000, random_state=1)"
      ],
      "metadata": {
        "id": "GYgDvjWeEALI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "i=0\n",
        "st1=' '\n",
        "fi_str=[]\n",
        "all_pos_wrds=[] \n",
        "all_neg_wrds=[] \n",
        "s1=''\n",
        "for ke in final['Reviews'].values:\n",
        "    fil_se=[]\n",
        "    try:\n",
        "      for w in ke.split():\n",
        "          for c in clpunc(w).split():\n",
        "              if((c.isalpha()) & (len(c)>2)):    \n",
        "                  if(c.lower() not in st):\n",
        "                      s=(sno.stem(c.lower())).encode('utf8')\n",
        "                      fil_se.append(s)\n",
        "                      if (final['RatingPosNeg'].values)[i] == 'positive': \n",
        "                          all_pos_wrds.append(s) \n",
        "                      if(final['RatingPosNeg'].values)[i] == 'negative':\n",
        "                          all_neg_wrds.append(s) \n",
        "                  else:\n",
        "                      continue\n",
        "              else:\n",
        "                  continue \n",
        "      str1 = b\" \".join(fil_se)\n",
        "      \n",
        "      fi_str.append(str1)\n",
        "      i+=1\n",
        "    except AttributeError as e:\n",
        "      fi_str.append('')\n",
        "      i+=1"
      ],
      "metadata": {
        "id": "oqqHEnhFEJDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final['CleanedText']=fi_str \n",
        "final['CleanedText']=final['CleanedText'].str.decode(\"utf-8\")\n",
        "final = final.fillna('')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "ct_vect = CountVectorizer()\n",
        "bo = ct_vect.fit_transform(final['CleanedText'].values)\n",
        "print(bo.shape)"
      ],
      "metadata": {
        "id": "GKgai_7VEDCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalterm = ct_vect.get_feature_names()"
      ],
      "metadata": {
        "id": "DR6EByO4ENh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "mod = KMeans(n_clusters=10,init='k-means++', n_init=10, max_iter=300, tol=0.0001, verbose=0, random_state=99, copy_x=True, algorithm='auto')\n",
        "mod.fit(bo)"
      ],
      "metadata": {
        "id": "uA_Uehd8EUZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nwlab = mod.labels_\n",
        "cluster_center=mod.cluster_centers_"
      ],
      "metadata": {
        "id": "UCscu0EJEW1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "silhouette_score = metrics.silhouette_score(bo, lab, metric='euclidean')\n",
        "silhouette_score"
      ],
      "metadata": {
        "id": "gMWgt0dQEZ50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datafrme = final\n",
        "datafrme['Bow Clus Label'] = mod.labels_ \n",
        "datafrme.head(2)\n"
      ],
      "metadata": {
        "id": "DA0tYtoaEaC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datafrme.groupby(['Bow Clus Label'])['Reviews'].count()"
      ],
      "metadata": {
        "id": "G8y-AULTEaPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top terms per cluster:\")\n",
        "ord_centroids = mod.cluster_centers_.argsort()[:, ::-1]\n",
        "for i1 in range(10):\n",
        "    print(\"Cluster %d:\" % i1, end='')\n",
        "    for i in ord_centroids[i1, :10]:\n",
        "        print(' %s' % term[i], end='')\n",
        "        print()"
      ],
      "metadata": {
        "id": "CkmDNwvUEj4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar([x1 for x1 in range(10)], dfrme.groupby(['Bow Clus Label'])['Reviews'].count(), alpha = 0.4)\n",
        "plt.title('KMeans cluster points')\n",
        "plt.xlabel(\"Cluster number\")\n",
        "plt.ylabel(\"Number of points\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vV34KdrIEomc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "idatafrm_vect = TfidfVectorizer()\n",
        "idatafrm = idatafrm_vect.fit_transform(final['CleanedText'].values)\n",
        "idatafrm.shape"
      ],
      "metadata": {
        "id": "1psNEMe9ErAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "mod = KMeans(n_clusters = 10,random_state=99)\n",
        "mod.fit(idf)"
      ],
      "metadata": {
        "id": "GHY-Akv3ErsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab_tf = mod.labels_\n",
        "cluster_center_tf=mod.cluster_centers_\n",
        "cluster_center_tf\n"
      ],
      "metadata": {
        "id": "q9fSqSqyErzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ter = idf_vect.get_feature_names()\n",
        "ter[1:10]"
      ],
      "metadata": {
        "id": "_ILhfpiqEsBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "silhouette_score_tf = metrics.silhouette_score(idf, lab_tf, metric='euclidean')\n",
        "silhouette_score_tf"
      ],
      "metadata": {
        "id": "Oiw-seteE3QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "silhouette_score_tf = metrics.silhouette_score(idf, lab_tf, metric='euclidean')\n",
        "silhouette_score_tf"
      ],
      "metadata": {
        "id": "tVv_DSjpE5sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfrme1 = dfrme\n",
        "dfrme1['Tfidf Clus Label'] = mod.labels_\n",
        "dfrme1.head(5)"
      ],
      "metadata": {
        "id": "aHR_40pYE53J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfrme1.groupby(['Tfidf Clus Label'])['Reviews'].count()\n"
      ],
      "metadata": {
        "id": "mEoZE6aME6CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = mod.cluster_centers_.argsort()[:, ::-1]\n",
        "for i1 in range(10):\n",
        "    print(\"Cluster %d:\" % i1, end='')\n",
        "    for i in order_centroids[i1, :10]:\n",
        "        print(' %s' % ter[i], end='')\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "Z8sYYAXBFNdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar([x1 for x1 in range(10)], dfrme1.groupby(['Tfidf Clus Label'])['Reviews'].count(), alpha = 0.4)\n",
        "plt.title('TFID cluster points')\n",
        "plt.xlabel(\"Cluster number\")\n",
        "plt.ylabel(\"Number of points\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bTGEO_OAFNmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i1=0\n",
        "lst_of_sent=[]\n",
        "for se in final['CleanedText'].values:\n",
        "    lst_of_sent.append(se.split())"
      ],
      "metadata": {
        "id": "0UnBOzCHFNw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i1=0\n",
        "lst_sent_tr=[]\n",
        "for s in final['CleanedText'].values:\n",
        "    filter_sen=[]\n",
        "    for w in se.split():\n",
        "        for cle in clpunc(w).split():\n",
        "            if(cle.isalpha()):    \n",
        "                filter_sen.append(cle.lower())\n",
        "            else:\n",
        "                continue \n",
        "    lst_sent_tr.append(filter_sen)\n",
        "print(lst_sent_tr)"
      ],
      "metadata": {
        "id": "fjHNfaNQFWoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "w2v_mod=gensim.models.Word2Vec(lst_sent_tr,size=100, workers=4)"
      ],
      "metadata": {
        "id": "kNBFV0Z8Faoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "se_vecs = []; \n",
        "for se in lst_sent_tr: \n",
        "    se_vec = np.zeros(100) \n",
        "    cnt_words =0;\n",
        "    for word in se: \n",
        "        try:\n",
        "            vec = w2v_mod.wv[word]\n",
        "            se_vec += vec\n",
        "            cnt_words += 1\n",
        "        except:\n",
        "            pass\n",
        "    se_vec /= cnt_words\n",
        "    se_vecs.append(se_vec)\n",
        "se_vecs = np.array(se_vecs)\n",
        "se_vecs = np.nan_to_num(se_vecs)\n",
        "se_vecs.shape"
      ],
      "metadata": {
        "id": "IisQbWiVFa5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_clu = [x1 for x1 in range(3,11)]\n",
        "n_clu"
      ],
      "metadata": {
        "id": "G_T2uOq7FbCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squared_errors = []\n",
        "for cluster in n_clu:\n",
        "    kmeans = KMeans(n_clusters = cluster).fit(se_vecs)\n",
        "    squared_errors.append(kmeans.inertia_)     \n",
        "optimal_clusters = np.argmin(squared_errors) + 2  \n",
        "print (\"The optimal number of clusters obtained is - \", optimal_clusters)\n",
        "print (\"The loss for optimal cluster is - \", min(squared_errors))"
      ],
      "metadata": {
        "id": "_jTJxbB5FngR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "model2 = KMeans(n_clusters = optimal_clusters)\n",
        "model2.fit(se_vecs)"
      ],
      "metadata": {
        "id": "mfo3tZTjFoZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_cluster_pred=model2.predict(se_vecs)\n",
        "word_cluster_pred_2=model2.labels_\n",
        "word_cluster_center=model2.cluster_centers_\n",
        "word_cluster_center[1:2]"
      ],
      "metadata": {
        "id": "6HGPJiO7Foiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datafrm = dfrme1\n",
        "datafrm['AVG-W2V Clus Label'] = model2.labels_\n",
        "datafrm.head(2)"
      ],
      "metadata": {
        "id": "qxtNqJpEFvBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "mi_Pts = 2 * 100\n",
        "def low(nums, target): \n",
        "    l, r = 0, len(nums) - 1\n",
        "    while l <= r: \n",
        "        mi_tr = int(l + (r - l) / 2)\n",
        "        if nums[mi_tr] >= target:\n",
        "            r = mi_tr - 1\n",
        "        else:\n",
        "            l = mi_tr + 1\n",
        "    return l\n",
        "def compute200t(x, data):\n",
        "    dists1 = []\n",
        "    for val in data:\n",
        "        dist = np.sum((x - val) **2 ) \n",
        "        if(len(dists1) == 200 and dists1[199] > dist): \n",
        "            l = int(low_bound(dists1, dist)) \n",
        "            if l < 200 and l >= 0 and dists1[l] > dist:\n",
        "                dists1[l] = dist\n",
        "        else:\n",
        "            dists1.append(dist)\n",
        "            dists1.sort()\n",
        "    \n",
        "    return dists1[199]"
      ],
      "metadata": {
        "id": "cEAgqNzxFyOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "two_hund_neigh = []\n",
        "for v1 in se_vecs[:1500]:\n",
        "    two_hund_neigh.append( compute200thnearestneighbour(v1, se_vecs[:1500]) )\n",
        "two_hund_neigh.sort()"
      ],
      "metadata": {
        "id": "C6darUrfF1je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_DB = DBSCAN(eps = 5, min_samples = mi_Pts, n_jobs=-1)\n",
        "mod_DB.fit(se_vecs)\n"
      ],
      "metadata": {
        "id": "-QcfhJlYF45a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfrme_db = dafrme\n",
        "dfrme_db['AVG-W2V Clus Label'] = mod_DB.labels_\n",
        "dfrme_db.head(2)"
      ],
      "metadata": {
        "id": "aCV05QjqF46x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(dfrme_db['Tfidf Clus Label'])"
      ],
      "metadata": {
        "id": "Ku6R4QaoF5Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daframe.groupby(['AVG-W2V Clus Label'])['Reviews'].count()"
      ],
      "metadata": {
        "id": "uveBow3WGDoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  #took n=5 from dendrogram curve \n",
        "Agg=cluster.fit_predict(se_vecs)"
      ],
      "metadata": {
        "id": "-z2f0wHdGDz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dafrme_a = dfrme_db\n",
        "dafrme_a['AVG-W2V Clus Label'] = cluster.labels_\n",
        "dafrme_a.head(2)"
      ],
      "metadata": {
        "id": "ITmpfiYcGD5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dafrme_a.groupby(['AVG-W2V Clus Label'])['Reviews'].count()\n"
      ],
      "metadata": {
        "id": "Gq9K4m8ZGJR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0q9A0TIl0-J"
      },
      "source": [
        "In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV-Er2osl0-J"
      },
      "outputs": [],
      "source": [
        "Kmeans: After the data is first grouped by similarity, the data points of k numbers are randomly assigned,\n",
        " exposing the centroids of each group. The remaining data points are then mapped to the following closest centroid,\n",
        "  and this procedure is repeated within each of these groups until the information in the centroids changes and every data point is stored in the same group.\n",
        "\n",
        "DBSCAN: To aid distinguish among data that are identical in nature, data points are classified according on how similar they are,\n",
        " and areas with greater data densities are kept as a single group while areas with lower densities are saved as noise.\n",
        "  A density-based spatial clustering technique called DBSCAN\n",
        "\n",
        "Hierarchical clustering:Hierarchical clustering is another unsupervised machine learning algorithm,\n",
        " which is used to group the unlabeled datasets into a cluster\n",
        "\n",
        "\n",
        "Word2Vec:Word2Vec is a method to create word embeddings that pre-dates BERT.\n",
        "Word2Vec generates embeddings that are independent from context, so each word is represented as a single vector as opposed to multiple\n",
        "\n",
        "\n",
        "\n",
        "BERT:BERT or Bidirectional Encoder Representations from Transformers, is a technique that allows for bidirectional training of Transformers for \n",
        "natural language modeling tasks.Language models which are bidirectionally trained can learn deeper context from language than single-direction models\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}