{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavanvarma007/-Pavan_INFO5731_Spring2023/blob/main/Programming_Assignment_02_INFO_5505.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LiGyTgXkIUwQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing neural networks from scratch"
      ],
      "metadata": {
        "id": "7tKRC3X-0M5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "define the sigmoid function and its derivative function"
      ],
      "metadata": {
        "id": "8YYnOhcV0Zyf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WSPX34NkDa4e"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z) :\n",
        "  return 1.0 / (1 + np. exp(-z))\n",
        "def sigmoid_derivative(z) :\n",
        "  return sigmoid(z) * (1.0 - sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "define the training function, which takes \n",
        "in the training dataset, the number of units in the hidden layer and the \n",
        "number of iterations"
      ],
      "metadata": {
        "id": "aM7hEpIu0bbK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gpv0Q77EGQNp"
      },
      "outputs": [],
      "source": [
        "def train(X, y, n_hidden, learning_rate, n_iter):\n",
        "  m, n_input = X.shape\n",
        "  W1 = np.random.randn(n_input, n_hidden)\n",
        "  b1 = np.zeros((1, n_hidden))\n",
        "  W2 = np.random.randn(n_hidden, 1)\n",
        "  b2 = np.zeros((1, 1))\n",
        "  for i in range(1, n_iter+1):\n",
        "    Z2 = np.matmul(X, W1) + b1\n",
        "    A2 = sigmoid(Z2)\n",
        "    Z3 = np.matmul (A2, W2) + b2\n",
        "    A3 = Z3\n",
        "    dZ3 = A3 - y\n",
        "    dW2 = np.matmul(A2.T, dZ3)\n",
        "    db2 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "    dZ2 = np.matmul(dZ3, W2.T) * sigmoid_derivative (Z2)\n",
        "    dW1 = np.matmul(X.T, dZ2)\n",
        "    db1 = np.sum(dZ2, axis=0)\n",
        "    W2 = W2 - learning_rate * dW2 / m\n",
        "    b2 = b2 - learning_rate * db2 / m\n",
        "    W1 = W1 - learning_rate * dW1 / m\n",
        "    b1 = b1 - learning_rate * db1 / m\n",
        "    if i % 100 == 0:\n",
        "      cost = np.mean((y - A3) ** 2)\n",
        "      print ('Iteration %i, training loss: %f' % (i,cost))\n",
        "  model={\"W1\":W1, \"b1\":b1, \"W2\":W2 ,\"b2\":b2 }\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vnkz2T_X0lXC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AoqK4TR-INaY"
      },
      "outputs": [],
      "source": [
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5bRManLmHJYQ"
      },
      "outputs": [],
      "source": [
        "num_test = 10 # the last 10 samples as testing set\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing. StandardScaler ()\n",
        "X_train = data[: -num_test, :]\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "y_train = target[:-num_test].reshape(-1, 1)\n",
        "X_test = data[-num_test:, :]\n",
        "X_test = scaler.transform(X_test)\n",
        "y_test = target [-num_test: ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVPBuQHiIjuo",
        "outputId": "23f52a79-1de4-40f0-d0fc-d19cdf8f7cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 100, training loss: 13.471024\n",
            "Iteration 200, training loss: 9.581926\n",
            "Iteration 300, training loss: 7.993478\n",
            "Iteration 400, training loss: 6.899583\n",
            "Iteration 500, training loss: 6.230106\n",
            "Iteration 600, training loss: 5.768195\n",
            "Iteration 700, training loss: 5.430610\n",
            "Iteration 800, training loss: 5.165193\n",
            "Iteration 900, training loss: 4.941881\n",
            "Iteration 1000, training loss: 4.745280\n",
            "Iteration 1100, training loss: 4.567559\n",
            "Iteration 1200, training loss: 4.401209\n",
            "Iteration 1300, training loss: 4.237478\n",
            "Iteration 1400, training loss: 4.067532\n",
            "Iteration 1500, training loss: 3.867281\n",
            "Iteration 1600, training loss: 3.689823\n",
            "Iteration 1700, training loss: 3.546526\n",
            "Iteration 1800, training loss: 3.417678\n",
            "Iteration 1900, training loss: 3.298614\n",
            "Iteration 2000, training loss: 3.188356\n"
          ]
        }
      ],
      "source": [
        "n_hidden = 20\n",
        "learning_rate = 0.1\n",
        "n_iter = 2000\n",
        "model = train(X_train, y_train, n_hidden, learning_rate, n_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ppVMg8AxJI_1"
      },
      "outputs": [],
      "source": [
        "def predict(x, model) :\n",
        "  W1 = model['W1']\n",
        "  b1 = model['b1']\n",
        "  W2 = model['W2']\n",
        "  b2 = model['b2']\n",
        "  A2 = sigmoid(np.matmul(x, W1) + b1)\n",
        "  A3 = np. matmul (A2, W2) + b2\n",
        "  return A3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rUKDwOa9KF9h"
      },
      "outputs": [],
      "source": [
        "predictions=predict(X_test,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrTmbBOmKMVn",
        "outputId": "8d4a172d-bf0f-4094-d5bd-ac044a440d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[18.7618836 ]\n",
            " [18.62083296]\n",
            " [20.69629527]\n",
            " [19.22456916]\n",
            " [18.84648381]\n",
            " [23.1075823 ]\n",
            " [23.15488438]\n",
            " [30.68909331]\n",
            " [28.23864679]\n",
            " [23.15492525]]\n"
          ]
        }
      ],
      "source": [
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unsCW71lKUA1",
        "outputId": "570ff102-339b-4448-a70a-7815ec47bef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9 22.  11.9]\n"
          ]
        }
      ],
      "source": [
        "print(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing neural networks with scikit-learn"
      ],
      "metadata": {
        "id": "5ruLTvhw0roX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NR0rqPTtJvGi"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "nn_scikit = MLPRegressor (hidden_layer_sizes=(16, 8),activation='relu', solver= 'adam',learning_rate_init=0.001, random_state=42, max_iter=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T89FckNPJ58V",
        "outputId": "2a44b189-4c09-4f82-fd67-a7cc37e2e3e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16.79582331 18.55538023 21.07961496 19.21362606 18.50955771 23.5608387\n",
            " 22.27916529 27.11909153 24.70251262 22.05522035]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "nn_scikit.fit(X_train, y_train)\n",
        "predictions = nn_scikit.predict(X_test)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH_ZTRDQKrXO",
        "outputId": "47c7cbdc-5e52-4556-9e86-b847b68055ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13.933482332708795\n"
          ]
        }
      ],
      "source": [
        "print(np.mean((y_test - predictions)**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing neural networks with TensorFlow"
      ],
      "metadata": {
        "id": "VjnUbXeJ04Ak"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PwTRpN5QMesL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lqsK4ghSZyYC"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([keras.layers.Dense(units=20, activation='relu'),\n",
        "                          keras.layers.Dense(units=8,activation= 'relu'),\n",
        "                          keras.layers.Dense(units=1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5T_hoSVBZ2U1"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.02))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tQtywMNZ5Vt",
        "outputId": "404188c8-37e0-4b99-e785-767aa391209e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "16/16 [==============================] - 1s 3ms/step - loss: 511.1687\n",
            "Epoch 2/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 158.4610\n",
            "Epoch 3/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 55.7038\n",
            "Epoch 4/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.0970\n",
            "Epoch 5/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 25.8157\n",
            "Epoch 6/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 21.2937\n",
            "Epoch 7/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 19.0666\n",
            "Epoch 8/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 17.1438\n",
            "Epoch 9/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 15.0633\n",
            "Epoch 10/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 13.5139\n",
            "Epoch 11/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 12.3889\n",
            "Epoch 12/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 12.0364\n",
            "Epoch 13/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.4633\n",
            "Epoch 14/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 10.9588\n",
            "Epoch 15/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 10.4202\n",
            "Epoch 16/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 10.0615\n",
            "Epoch 17/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 10.0803\n",
            "Epoch 18/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.8569\n",
            "Epoch 19/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.6122\n",
            "Epoch 20/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 9.9457\n",
            "Epoch 21/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 9.8560\n",
            "Epoch 22/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.3421\n",
            "Epoch 23/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 9.1014\n",
            "Epoch 24/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 9.1586\n",
            "Epoch 25/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 9.3085\n",
            "Epoch 26/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.9083\n",
            "Epoch 27/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 9.4534\n",
            "Epoch 28/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 9.7015\n",
            "Epoch 29/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.6248\n",
            "Epoch 30/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.6919\n",
            "Epoch 31/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.4140\n",
            "Epoch 32/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.3284\n",
            "Epoch 33/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.1878\n",
            "Epoch 34/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.2771\n",
            "Epoch 35/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.5554\n",
            "Epoch 36/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.4925\n",
            "Epoch 37/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.0668\n",
            "Epoch 38/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.9806\n",
            "Epoch 39/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.3512\n",
            "Epoch 40/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.1768\n",
            "Epoch 41/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.8659\n",
            "Epoch 42/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.0291\n",
            "Epoch 43/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.2658\n",
            "Epoch 44/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.7766\n",
            "Epoch 45/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 8.6687\n",
            "Epoch 46/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.9513\n",
            "Epoch 47/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 7.7909\n",
            "Epoch 48/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.6901\n",
            "Epoch 49/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.8645\n",
            "Epoch 50/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.8068\n",
            "Epoch 51/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.9027\n",
            "Epoch 52/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.9677\n",
            "Epoch 53/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.2242\n",
            "Epoch 54/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 8.3844\n",
            "Epoch 55/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.6021\n",
            "Epoch 56/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.3945\n",
            "Epoch 57/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.4890\n",
            "Epoch 58/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.4318\n",
            "Epoch 59/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.1614\n",
            "Epoch 60/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.1705\n",
            "Epoch 61/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.5462\n",
            "Epoch 62/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.4604\n",
            "Epoch 63/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.1581\n",
            "Epoch 64/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.2644\n",
            "Epoch 65/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.3434\n",
            "Epoch 66/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.3580\n",
            "Epoch 67/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.4674\n",
            "Epoch 68/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.3803\n",
            "Epoch 69/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.6580\n",
            "Epoch 70/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.5678\n",
            "Epoch 71/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.0917\n",
            "Epoch 72/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.9840\n",
            "Epoch 73/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.9297\n",
            "Epoch 74/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.9804\n",
            "Epoch 75/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.2416\n",
            "Epoch 76/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.0922\n",
            "Epoch 77/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.1749\n",
            "Epoch 78/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.9234\n",
            "Epoch 79/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.7304\n",
            "Epoch 80/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.4864\n",
            "Epoch 81/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.0031\n",
            "Epoch 82/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.9566\n",
            "Epoch 83/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.3603\n",
            "Epoch 84/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.2827\n",
            "Epoch 85/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.8921\n",
            "Epoch 86/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.1285\n",
            "Epoch 87/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.6675\n",
            "Epoch 88/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.4902\n",
            "Epoch 89/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 6.7773\n",
            "Epoch 90/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.8990\n",
            "Epoch 91/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.6852\n",
            "Epoch 92/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.0251\n",
            "Epoch 93/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.1300\n",
            "Epoch 94/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.6596\n",
            "Epoch 95/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.5021\n",
            "Epoch 96/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.5151\n",
            "Epoch 97/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 7.3421\n",
            "Epoch 98/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.7988\n",
            "Epoch 99/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.7668\n",
            "Epoch 100/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.7099\n",
            "Epoch 101/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.7048\n",
            "Epoch 102/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.6715\n",
            "Epoch 103/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.9598\n",
            "Epoch 104/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.7245\n",
            "Epoch 105/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.3636\n",
            "Epoch 106/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.9307\n",
            "Epoch 107/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.5852\n",
            "Epoch 108/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.9716\n",
            "Epoch 109/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4553\n",
            "Epoch 110/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.6085\n",
            "Epoch 111/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.9990\n",
            "Epoch 112/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.7634\n",
            "Epoch 113/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4732\n",
            "Epoch 114/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.5381\n",
            "Epoch 115/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4074\n",
            "Epoch 116/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.7278\n",
            "Epoch 117/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.4022\n",
            "Epoch 118/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.5970\n",
            "Epoch 119/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4049\n",
            "Epoch 120/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4289\n",
            "Epoch 121/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2358\n",
            "Epoch 122/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4873\n",
            "Epoch 123/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2678\n",
            "Epoch 124/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.1619\n",
            "Epoch 125/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.1887\n",
            "Epoch 126/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.7710\n",
            "Epoch 127/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.1783\n",
            "Epoch 128/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.6171\n",
            "Epoch 129/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.3465\n",
            "Epoch 130/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.6342\n",
            "Epoch 131/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.0508\n",
            "Epoch 132/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.9704\n",
            "Epoch 133/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.8970\n",
            "Epoch 134/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.9475\n",
            "Epoch 135/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.8773\n",
            "Epoch 136/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.5047\n",
            "Epoch 137/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4262\n",
            "Epoch 138/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.3292\n",
            "Epoch 139/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0225\n",
            "Epoch 140/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.3398\n",
            "Epoch 141/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.3013\n",
            "Epoch 142/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0248\n",
            "Epoch 143/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2564\n",
            "Epoch 144/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0581\n",
            "Epoch 145/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0511\n",
            "Epoch 146/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0580\n",
            "Epoch 147/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.4056\n",
            "Epoch 148/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.8068\n",
            "Epoch 149/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.5510\n",
            "Epoch 150/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.1075\n",
            "Epoch 151/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9759\n",
            "Epoch 152/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.1783\n",
            "Epoch 153/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.7626\n",
            "Epoch 154/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.1942\n",
            "Epoch 155/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.8087\n",
            "Epoch 156/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.6906\n",
            "Epoch 157/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.5399\n",
            "Epoch 158/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.1133\n",
            "Epoch 159/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.1938\n",
            "Epoch 160/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.1538\n",
            "Epoch 161/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0999\n",
            "Epoch 162/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9109\n",
            "Epoch 163/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9593\n",
            "Epoch 164/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9253\n",
            "Epoch 165/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.8845\n",
            "Epoch 166/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2637\n",
            "Epoch 167/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2618\n",
            "Epoch 168/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.7590\n",
            "Epoch 169/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.0046\n",
            "Epoch 170/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9730\n",
            "Epoch 171/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9333\n",
            "Epoch 172/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.7642\n",
            "Epoch 173/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.7668\n",
            "Epoch 174/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.3503\n",
            "Epoch 175/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.5339\n",
            "Epoch 176/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.5810\n",
            "Epoch 177/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.4698\n",
            "Epoch 178/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 6.3482\n",
            "Epoch 179/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 6.5493\n",
            "Epoch 180/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.1356\n",
            "Epoch 181/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.4524\n",
            "Epoch 182/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2092\n",
            "Epoch 183/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2897\n",
            "Epoch 184/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.1727\n",
            "Epoch 185/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9067\n",
            "Epoch 186/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.9071\n",
            "Epoch 187/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.8558\n",
            "Epoch 188/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.8223\n",
            "Epoch 189/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.9489\n",
            "Epoch 190/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.0011\n",
            "Epoch 191/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.2728\n",
            "Epoch 192/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.0681\n",
            "Epoch 193/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.8886\n",
            "Epoch 194/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.2498\n",
            "Epoch 195/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6.1491\n",
            "Epoch 196/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 6.0070\n",
            "Epoch 197/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.6786\n",
            "Epoch 198/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.5625\n",
            "Epoch 199/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.9177\n",
            "Epoch 200/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.3818\n",
            "Epoch 201/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.4953\n",
            "Epoch 202/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.6340\n",
            "Epoch 203/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.5983\n",
            "Epoch 204/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.7822\n",
            "Epoch 205/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.4285\n",
            "Epoch 206/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.6029\n",
            "Epoch 207/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.4624\n",
            "Epoch 208/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.5851\n",
            "Epoch 209/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.4062\n",
            "Epoch 210/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.6111\n",
            "Epoch 211/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.3997\n",
            "Epoch 212/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.3417\n",
            "Epoch 213/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.4203\n",
            "Epoch 214/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.8293\n",
            "Epoch 215/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 5.4385\n",
            "Epoch 216/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.1892\n",
            "Epoch 217/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.2900\n",
            "Epoch 218/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 5.8520\n",
            "Epoch 219/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 5.2530\n",
            "Epoch 220/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.2176\n",
            "Epoch 221/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.0886\n",
            "Epoch 222/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.9113\n",
            "Epoch 223/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 6.1932\n",
            "Epoch 224/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.7232\n",
            "Epoch 225/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.9618\n",
            "Epoch 226/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.9334\n",
            "Epoch 227/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.4464\n",
            "Epoch 228/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.9802\n",
            "Epoch 229/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.2785\n",
            "Epoch 230/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.2142\n",
            "Epoch 231/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.2204\n",
            "Epoch 232/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.6732\n",
            "Epoch 233/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.2997\n",
            "Epoch 234/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.2052\n",
            "Epoch 235/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.0305\n",
            "Epoch 236/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.7864\n",
            "Epoch 237/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.2185\n",
            "Epoch 238/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.3147\n",
            "Epoch 239/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.1508\n",
            "Epoch 240/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.9593\n",
            "Epoch 241/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.8475\n",
            "Epoch 242/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 5.1358\n",
            "Epoch 243/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.1415\n",
            "Epoch 244/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.0913\n",
            "Epoch 245/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.4469\n",
            "Epoch 246/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 5.5808\n",
            "Epoch 247/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.0080\n",
            "Epoch 248/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 4.7053\n",
            "Epoch 249/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 4.7202\n",
            "Epoch 250/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.6518\n",
            "Epoch 251/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.8050\n",
            "Epoch 252/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.6606\n",
            "Epoch 253/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.9606\n",
            "Epoch 254/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.9006\n",
            "Epoch 255/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.6600\n",
            "Epoch 256/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.7231\n",
            "Epoch 257/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.2058\n",
            "Epoch 258/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.0160\n",
            "Epoch 259/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.8834\n",
            "Epoch 260/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.8405\n",
            "Epoch 261/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 4.5995\n",
            "Epoch 262/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.5477\n",
            "Epoch 263/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.4311\n",
            "Epoch 264/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.6136\n",
            "Epoch 265/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.8113\n",
            "Epoch 266/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.8185\n",
            "Epoch 267/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.5890\n",
            "Epoch 268/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.4938\n",
            "Epoch 269/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.5810\n",
            "Epoch 270/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.4693\n",
            "Epoch 271/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.8834\n",
            "Epoch 272/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.9045\n",
            "Epoch 273/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.7640\n",
            "Epoch 274/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 4.8313\n",
            "Epoch 275/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 4.9796\n",
            "Epoch 276/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.9673\n",
            "Epoch 277/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.4844\n",
            "Epoch 278/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.8298\n",
            "Epoch 279/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.4857\n",
            "Epoch 280/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.5042\n",
            "Epoch 281/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.3513\n",
            "Epoch 282/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.4369\n",
            "Epoch 283/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.7480\n",
            "Epoch 284/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.8702\n",
            "Epoch 285/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.9451\n",
            "Epoch 286/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.8343\n",
            "Epoch 287/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.3699\n",
            "Epoch 288/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.6667\n",
            "Epoch 289/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.2866\n",
            "Epoch 290/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.1614\n",
            "Epoch 291/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 5.1920\n",
            "Epoch 292/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.2860\n",
            "Epoch 293/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.6763\n",
            "Epoch 294/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.5637\n",
            "Epoch 295/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.1498\n",
            "Epoch 296/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.5677\n",
            "Epoch 297/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.5492\n",
            "Epoch 298/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 5.5182\n",
            "Epoch 299/300\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.8806\n",
            "Epoch 300/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 4.4664\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f65e588bac0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model.fit (X_train,y_train,epochs=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gBJJYIEZ8jb",
        "outputId": "1ff361dd-4343-41f8-f781-fa3342adc601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 109ms/step\n",
            "[17.303087 18.679602 20.435934 18.737612 19.050352 26.506218 23.52595\n",
            " 30.40733  28.05226  23.048542]\n",
            "24.175623030929348\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(X_test)[:, 0]\n",
        "print(predictions)\n",
        "print(np.mean((y_test - predictions) ** 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Predicting stock prices with neural networks"
      ],
      "metadata": {
        "id": "KPP90ynK1BOb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4yjLr6d0bldW"
      },
      "outputs": [],
      "source": [
        "data_raw = pd.read_csv('/content/dataset.csv', index_col='Date')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "geYlB__2dctv"
      },
      "outputs": [],
      "source": [
        "def add_original_feature(df, df_new):\n",
        "    df_new['open'] = df[ 'Open']\n",
        "    df_new['open_1'] = df['Open'].shift (1)\n",
        "    df_new['close_1'] = df['Close'].shift(1)\n",
        "    df_new['high_1'] = df['High'].shift (1)\n",
        "    df_new['low_1'] = df['Low' ].shift(1)\n",
        "    df_new['volume_1'] = df[ 'Volume'].shift(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5iiPVh19eECd"
      },
      "outputs": [],
      "source": [
        "def add_avg_price(df, df_new) :\n",
        "    df_new['avg_price_5'] = df['Close'].rolling(5).mean().shift(1)\n",
        "    df_new['avg_price_30'] = df['Close'].rolling(21) .mean().shift (1)\n",
        "    df_new['avg_price_365'] = df['Close'].rolling(252).mean().shift(1)\n",
        "    df_new['ratio_avg_price_5_30'] = df_new['avg_price_5'] / df_new['avg_price_30']\n",
        "    df_new['ratio_avg_price_5_365'] = df_new['avg_price_5'] / df_new['avg_price_365']\n",
        "    df_new['ratio_avg_price_30_365'] = df_new[ 'avg_price_30'] / df_new['avg_price_365']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KmfSRqNWb6dD"
      },
      "outputs": [],
      "source": [
        "def add_avg_volume(df, df_new):\n",
        "    df_new['avg_volume_5'] =df['Volume'].rolling(5).mean().shift(1)\n",
        "    df_new['avg_volume_30'] =df['Volume'].rolling(21).mean().shift(1)\n",
        "    df_new['avg_volume_365'] =df['Volume'].rolling(252).mean().shift (1)\n",
        "    df_new['ratio_avg_volume_5_30']=df_new['avg_volume_5'] / df_new['avg_volume_30']\n",
        "    df_new['ratio_avg_volume_5_365'] =df_new['avg_volume_5'] / df_new['avg_volume_365']\n",
        "    df_new['ratio_avg volume_30_365'] =df_new['avg_volume_30'] / df_new['avg_volume_365']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BrHCShJVb9k3"
      },
      "outputs": [],
      "source": [
        "def add_std_price(df, df_new):\n",
        "    df_new['std_price_5'] =df['Close'].rolling (5).std().shift(1)\n",
        "    df_new['std_price_30'] =df['Close'].rolling(21).std().shift(1)\n",
        "    df_new['std_price_365'] =df['Close'].rolling(252).std().shift(1)\n",
        "    df_new['ratio_std_price_5_30']=df_new['std_price_5'] / df_new['std_price_30']\n",
        "    df_new['ratio_std_price_5_365'] =df_new['std_price_5'] / df_new['std_price_365'] \n",
        "    df_new['ratio_std_price_30_365'] =df_new['std_price_30'] / df_new['std_price_365']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dN19_sK5cAyd"
      },
      "outputs": [],
      "source": [
        "def add_std_volume(df, df_new):\n",
        "    df_new['std_volume_5'] =df['Volume'].rolling(5).std().shift(1)\n",
        "    df_new['std_volume_30'] =df['Volume'].rolling(21) .std().shift(1)\n",
        "    df_new['std_volume_365'] =df['Volume'].rolling(252).std().shift(1)\n",
        "    df_new['ratio_std_volume_5_30'] =df_new['std_volume_5'] / df_new['std_volume_30']\n",
        "    df_new['ratio_std_volume_5_365'] =df_new['std_volume_5'] / df_new['std_volume_365']\n",
        "    df_new['ratio_std_volume_30_365'] =df_new['std_volume_30'] / df_new['std_volume_365']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DtmsViPQcDoV"
      },
      "outputs": [],
      "source": [
        "def add_return_feature(df, df_new):\n",
        "    df_new['return_1'] = ((df['Close'] - df['Close']. shift (1))/ df['Close'].shift(1)).shift(1)\n",
        "    df_new['return_5'] = ((df['Close'] - df['Close'].shift(5))/ df[ 'Close'].shift (5)).shift(1)\n",
        "    df_new['return_30'] = ((df['Close'] -df['Close'].shift (21)) / df['Close'].shift (21)) .shift(1)\n",
        "    df_new['return_365'] = ((df['Close'] -df['Close'].shift (252)) / df['Close'].shift (252)) .shift(1)\n",
        "    df_new['moving_ave_5'] =df_new['return_1'].rolling(5).mean().shift(1)\n",
        "    df_new['moving_avg_30'] =df_new['return_1'].rolling(21).mean().shift(1)\n",
        "    df_new['moving_avg_365'] =df_new['return_1'].rolling(252).mean().shift(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KXSquojucG58"
      },
      "outputs": [],
      "source": [
        "def generate_features (df):\n",
        "    df_new = pd.DataFrame ()\n",
        "    add_original_feature(df, df_new)\n",
        "    add_avg_price(df, df_new)\n",
        "    add_avg_volume(df, df_new)\n",
        "    add_std_price(df, df_new)\n",
        "    add_std_volume(df, df_new)\n",
        "    add_return_feature(df, df_new)\n",
        "    df_new['close'] = df['Close']\n",
        "    df_new = df_new.dropna(axis=0)\n",
        "    return df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JymAOe0EcJtt"
      },
      "outputs": [],
      "source": [
        "data = generate_features (data_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "7PTLCCTzcNAe",
        "outputId": "912e9457-619c-44c4-f513-2db34f35b897"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                open    open_1   close_1    high_1     low_1     volume_1  \\\n",
              "Date                                                                        \n",
              "1930-12-05    181.00    184.10    181.00    184.10    179.80     179324.0   \n",
              "1930-12-06    180.60    181.00    181.10    181.70    177.30     179324.0   \n",
              "1930-12-08    178.00    180.60    178.40    180.60    177.60     159699.0   \n",
              "1930-12-09    176.10    178.00    176.10    178.00    174.30     223309.0   \n",
              "1930-12-10    176.50    176.10    176.50    178.20    173.90     239098.0   \n",
              "...              ...       ...       ...       ...       ...          ...   \n",
              "2023-03-06  33425.32  33076.33  33390.97  33405.82  33008.41  304499671.0   \n",
              "2023-03-07  33428.31  33425.32  33431.44  33572.22  33383.47  332530856.0   \n",
              "2023-03-08  32872.08  33428.31  32856.46  33453.25  32838.21  302316239.0   \n",
              "2023-03-09  32876.83  32872.08  32798.40  32903.44  32612.70  244719997.0   \n",
              "2023-03-10  32185.14  32876.83  32254.86  32990.46  32190.60  319329492.0   \n",
              "\n",
              "            avg_price_5  avg_price_30  avg_price_365  ratio_avg_price_5_30  \\\n",
              "Date                                                                         \n",
              "1930-12-05      184.158    184.816190     239.677937              0.996439   \n",
              "1930-12-06      183.700    184.849524     239.339048              0.993781   \n",
              "1930-12-08      182.280    184.582857     238.981508              0.987524   \n",
              "1930-12-09      180.140    184.078095     238.600556              0.978606   \n",
              "1930-12-10      178.620    183.887619     238.235079              0.971354   \n",
              "...                 ...           ...            ...                   ...   \n",
              "2023-03-06    32920.434  33537.156667   32674.986389              0.981611   \n",
              "2023-03-07    33028.904  33507.513810   32673.545040              0.985716   \n",
              "2023-03-08    33068.856  33456.582857   32670.535754              0.988411   \n",
              "2023-03-09    33096.168  33404.553333   32670.460437              0.990768   \n",
              "2023-03-10    32946.426  33313.990000   32668.961310              0.988967   \n",
              "\n",
              "            ...  ratio_std_volume_5_365  ratio_std_volume_30_365  return_1  \\\n",
              "Date        ...                                                              \n",
              "1930-12-05  ...                0.262171                 0.460315 -0.016839   \n",
              "1930-12-06  ...                0.181719                 0.404141  0.000552   \n",
              "1930-12-08  ...                0.124114                 0.401101 -0.014909   \n",
              "1930-12-09  ...                0.214120                 0.307919 -0.012892   \n",
              "1930-12-10  ...                0.226450                 0.307424  0.002271   \n",
              "...         ...                     ...                      ...       ...   \n",
              "2023-03-06  ...                0.315063                 0.529048  0.011738   \n",
              "2023-03-07  ...                0.150927                 0.416379  0.001212   \n",
              "2023-03-08  ...                0.160361                 0.308294 -0.017199   \n",
              "2023-03-09  ...                0.376121                 0.347318 -0.001767   \n",
              "2023-03-10  ...                0.383035                 0.324699 -0.016572   \n",
              "\n",
              "            return_5  return_30  return_365  moving_ave_5  moving_avg_30  \\\n",
              "Date                                                                       \n",
              "1930-12-05 -0.013086   0.020869   -0.325633      0.003565       0.002995   \n",
              "1930-12-06 -0.012487   0.003880   -0.320450     -0.002567       0.001094   \n",
              "1930-12-08 -0.038275  -0.030435   -0.335568     -0.002445       0.000288   \n",
              "1930-12-09 -0.057281  -0.056776   -0.352811     -0.007728      -0.001372   \n",
              "1930-12-10 -0.041282  -0.022161   -0.342889     -0.011708      -0.002685   \n",
              "...              ...        ...         ...           ...            ...   \n",
              "2023-03-06  0.017493  -0.020590   -0.014764     -0.000882      -0.001507   \n",
              "2023-03-07  0.016490  -0.018280   -0.010748      0.003498      -0.000958   \n",
              "2023-03-08  0.006117  -0.031526   -0.022560      0.003301      -0.000846   \n",
              "2023-03-09  0.004181  -0.032239   -0.000578      0.001274      -0.001486   \n",
              "2023-03-10 -0.022686  -0.055680   -0.011577      0.000889      -0.001521   \n",
              "\n",
              "            moving_avg_365     close  \n",
              "Date                                  \n",
              "1930-12-05       -0.001334    181.10  \n",
              "1930-12-06       -0.001420    178.40  \n",
              "1930-12-08       -0.001390    176.10  \n",
              "1930-12-09       -0.001479    176.50  \n",
              "1930-12-10       -0.001583    174.00  \n",
              "...                    ...       ...  \n",
              "2023-03-06        0.000040  33431.44  \n",
              "2023-03-07        0.000016  32856.46  \n",
              "2023-03-08        0.000032  32798.40  \n",
              "2023-03-09       -0.000015  32254.86  \n",
              "2023-03-10        0.000072  31909.64  \n",
              "\n",
              "[24147 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f6ebc297-517b-482f-8405-f6a8221a7b54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>open_1</th>\n",
              "      <th>close_1</th>\n",
              "      <th>high_1</th>\n",
              "      <th>low_1</th>\n",
              "      <th>volume_1</th>\n",
              "      <th>avg_price_5</th>\n",
              "      <th>avg_price_30</th>\n",
              "      <th>avg_price_365</th>\n",
              "      <th>ratio_avg_price_5_30</th>\n",
              "      <th>...</th>\n",
              "      <th>ratio_std_volume_5_365</th>\n",
              "      <th>ratio_std_volume_30_365</th>\n",
              "      <th>return_1</th>\n",
              "      <th>return_5</th>\n",
              "      <th>return_30</th>\n",
              "      <th>return_365</th>\n",
              "      <th>moving_ave_5</th>\n",
              "      <th>moving_avg_30</th>\n",
              "      <th>moving_avg_365</th>\n",
              "      <th>close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1930-12-05</th>\n",
              "      <td>181.00</td>\n",
              "      <td>184.10</td>\n",
              "      <td>181.00</td>\n",
              "      <td>184.10</td>\n",
              "      <td>179.80</td>\n",
              "      <td>179324.0</td>\n",
              "      <td>184.158</td>\n",
              "      <td>184.816190</td>\n",
              "      <td>239.677937</td>\n",
              "      <td>0.996439</td>\n",
              "      <td>...</td>\n",
              "      <td>0.262171</td>\n",
              "      <td>0.460315</td>\n",
              "      <td>-0.016839</td>\n",
              "      <td>-0.013086</td>\n",
              "      <td>0.020869</td>\n",
              "      <td>-0.325633</td>\n",
              "      <td>0.003565</td>\n",
              "      <td>0.002995</td>\n",
              "      <td>-0.001334</td>\n",
              "      <td>181.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1930-12-06</th>\n",
              "      <td>180.60</td>\n",
              "      <td>181.00</td>\n",
              "      <td>181.10</td>\n",
              "      <td>181.70</td>\n",
              "      <td>177.30</td>\n",
              "      <td>179324.0</td>\n",
              "      <td>183.700</td>\n",
              "      <td>184.849524</td>\n",
              "      <td>239.339048</td>\n",
              "      <td>0.993781</td>\n",
              "      <td>...</td>\n",
              "      <td>0.181719</td>\n",
              "      <td>0.404141</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>-0.012487</td>\n",
              "      <td>0.003880</td>\n",
              "      <td>-0.320450</td>\n",
              "      <td>-0.002567</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>-0.001420</td>\n",
              "      <td>178.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1930-12-08</th>\n",
              "      <td>178.00</td>\n",
              "      <td>180.60</td>\n",
              "      <td>178.40</td>\n",
              "      <td>180.60</td>\n",
              "      <td>177.60</td>\n",
              "      <td>159699.0</td>\n",
              "      <td>182.280</td>\n",
              "      <td>184.582857</td>\n",
              "      <td>238.981508</td>\n",
              "      <td>0.987524</td>\n",
              "      <td>...</td>\n",
              "      <td>0.124114</td>\n",
              "      <td>0.401101</td>\n",
              "      <td>-0.014909</td>\n",
              "      <td>-0.038275</td>\n",
              "      <td>-0.030435</td>\n",
              "      <td>-0.335568</td>\n",
              "      <td>-0.002445</td>\n",
              "      <td>0.000288</td>\n",
              "      <td>-0.001390</td>\n",
              "      <td>176.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1930-12-09</th>\n",
              "      <td>176.10</td>\n",
              "      <td>178.00</td>\n",
              "      <td>176.10</td>\n",
              "      <td>178.00</td>\n",
              "      <td>174.30</td>\n",
              "      <td>223309.0</td>\n",
              "      <td>180.140</td>\n",
              "      <td>184.078095</td>\n",
              "      <td>238.600556</td>\n",
              "      <td>0.978606</td>\n",
              "      <td>...</td>\n",
              "      <td>0.214120</td>\n",
              "      <td>0.307919</td>\n",
              "      <td>-0.012892</td>\n",
              "      <td>-0.057281</td>\n",
              "      <td>-0.056776</td>\n",
              "      <td>-0.352811</td>\n",
              "      <td>-0.007728</td>\n",
              "      <td>-0.001372</td>\n",
              "      <td>-0.001479</td>\n",
              "      <td>176.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1930-12-10</th>\n",
              "      <td>176.50</td>\n",
              "      <td>176.10</td>\n",
              "      <td>176.50</td>\n",
              "      <td>178.20</td>\n",
              "      <td>173.90</td>\n",
              "      <td>239098.0</td>\n",
              "      <td>178.620</td>\n",
              "      <td>183.887619</td>\n",
              "      <td>238.235079</td>\n",
              "      <td>0.971354</td>\n",
              "      <td>...</td>\n",
              "      <td>0.226450</td>\n",
              "      <td>0.307424</td>\n",
              "      <td>0.002271</td>\n",
              "      <td>-0.041282</td>\n",
              "      <td>-0.022161</td>\n",
              "      <td>-0.342889</td>\n",
              "      <td>-0.011708</td>\n",
              "      <td>-0.002685</td>\n",
              "      <td>-0.001583</td>\n",
              "      <td>174.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-06</th>\n",
              "      <td>33425.32</td>\n",
              "      <td>33076.33</td>\n",
              "      <td>33390.97</td>\n",
              "      <td>33405.82</td>\n",
              "      <td>33008.41</td>\n",
              "      <td>304499671.0</td>\n",
              "      <td>32920.434</td>\n",
              "      <td>33537.156667</td>\n",
              "      <td>32674.986389</td>\n",
              "      <td>0.981611</td>\n",
              "      <td>...</td>\n",
              "      <td>0.315063</td>\n",
              "      <td>0.529048</td>\n",
              "      <td>0.011738</td>\n",
              "      <td>0.017493</td>\n",
              "      <td>-0.020590</td>\n",
              "      <td>-0.014764</td>\n",
              "      <td>-0.000882</td>\n",
              "      <td>-0.001507</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>33431.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-07</th>\n",
              "      <td>33428.31</td>\n",
              "      <td>33425.32</td>\n",
              "      <td>33431.44</td>\n",
              "      <td>33572.22</td>\n",
              "      <td>33383.47</td>\n",
              "      <td>332530856.0</td>\n",
              "      <td>33028.904</td>\n",
              "      <td>33507.513810</td>\n",
              "      <td>32673.545040</td>\n",
              "      <td>0.985716</td>\n",
              "      <td>...</td>\n",
              "      <td>0.150927</td>\n",
              "      <td>0.416379</td>\n",
              "      <td>0.001212</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>-0.018280</td>\n",
              "      <td>-0.010748</td>\n",
              "      <td>0.003498</td>\n",
              "      <td>-0.000958</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>32856.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-08</th>\n",
              "      <td>32872.08</td>\n",
              "      <td>33428.31</td>\n",
              "      <td>32856.46</td>\n",
              "      <td>33453.25</td>\n",
              "      <td>32838.21</td>\n",
              "      <td>302316239.0</td>\n",
              "      <td>33068.856</td>\n",
              "      <td>33456.582857</td>\n",
              "      <td>32670.535754</td>\n",
              "      <td>0.988411</td>\n",
              "      <td>...</td>\n",
              "      <td>0.160361</td>\n",
              "      <td>0.308294</td>\n",
              "      <td>-0.017199</td>\n",
              "      <td>0.006117</td>\n",
              "      <td>-0.031526</td>\n",
              "      <td>-0.022560</td>\n",
              "      <td>0.003301</td>\n",
              "      <td>-0.000846</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>32798.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-09</th>\n",
              "      <td>32876.83</td>\n",
              "      <td>32872.08</td>\n",
              "      <td>32798.40</td>\n",
              "      <td>32903.44</td>\n",
              "      <td>32612.70</td>\n",
              "      <td>244719997.0</td>\n",
              "      <td>33096.168</td>\n",
              "      <td>33404.553333</td>\n",
              "      <td>32670.460437</td>\n",
              "      <td>0.990768</td>\n",
              "      <td>...</td>\n",
              "      <td>0.376121</td>\n",
              "      <td>0.347318</td>\n",
              "      <td>-0.001767</td>\n",
              "      <td>0.004181</td>\n",
              "      <td>-0.032239</td>\n",
              "      <td>-0.000578</td>\n",
              "      <td>0.001274</td>\n",
              "      <td>-0.001486</td>\n",
              "      <td>-0.000015</td>\n",
              "      <td>32254.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-10</th>\n",
              "      <td>32185.14</td>\n",
              "      <td>32876.83</td>\n",
              "      <td>32254.86</td>\n",
              "      <td>32990.46</td>\n",
              "      <td>32190.60</td>\n",
              "      <td>319329492.0</td>\n",
              "      <td>32946.426</td>\n",
              "      <td>33313.990000</td>\n",
              "      <td>32668.961310</td>\n",
              "      <td>0.988967</td>\n",
              "      <td>...</td>\n",
              "      <td>0.383035</td>\n",
              "      <td>0.324699</td>\n",
              "      <td>-0.016572</td>\n",
              "      <td>-0.022686</td>\n",
              "      <td>-0.055680</td>\n",
              "      <td>-0.011577</td>\n",
              "      <td>0.000889</td>\n",
              "      <td>-0.001521</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>31909.64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24147 rows  38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f6ebc297-517b-482f-8405-f6a8221a7b54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f6ebc297-517b-482f-8405-f6a8221a7b54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f6ebc297-517b-482f-8405-f6a8221a7b54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "X9sE4Z1wcT34"
      },
      "outputs": [],
      "source": [
        "start_train = '1988-01-01'\n",
        "end_train = '2018-12-31'\n",
        "start_test = '2019-01-01'\n",
        "end_test = '2019-12-31'\n",
        "data_train = data.loc[start_train:end_train]\n",
        "X_train = data_train.drop('close', axis=1).values\n",
        "y_train = data_train['close'].values\n",
        "data_test = data.loc[start_test:end_test]\n",
        "X_test = data_test.drop('close', axis=1).values\n",
        "y_test = data_test ['close'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KrDKfcjbhQuH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooNNefrThtFn",
        "outputId": "a41d1271-ef32-4589-d221-4bf4f73b1f20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2015.3 ,  2031.5 ,  2037.8 , ..., 23138.82, 23062.4 , 23327.46])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "X_scaled_train = scaler.fit_transform(X_train)\n",
        "X_scaled_test = scaler.transform(X_test)\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "r5tI9GuthwdC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "model = Sequential([\n",
        "Dense (units=32, activation='relu'),\n",
        "Dense (units=1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "nChHEIDbh0nh"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i2dUqJdh6QG",
        "outputId": "62c85ae3-ea47-4a14-cec5-5d3bcd966fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 27908782.0000\n",
            "Epoch 2/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 1850590.6250\n",
            "Epoch 3/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 539293.1250\n",
            "Epoch 4/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 188518.0000\n",
            "Epoch 5/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 81358.6875\n",
            "Epoch 6/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 45569.5781\n",
            "Epoch 7/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 31609.2012\n",
            "Epoch 8/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 25805.3438\n",
            "Epoch 9/100\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 24370.2578\n",
            "Epoch 10/100\n",
            "245/245 [==============================] - 2s 8ms/step - loss: 23580.1484\n",
            "Epoch 11/100\n",
            "245/245 [==============================] - 1s 4ms/step - loss: 24060.1562\n",
            "Epoch 12/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 22119.0449\n",
            "Epoch 13/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 22958.8945\n",
            "Epoch 14/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 28052.7773\n",
            "Epoch 15/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 25808.0898\n",
            "Epoch 16/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 24937.5391\n",
            "Epoch 17/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 26014.5020\n",
            "Epoch 18/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 24441.9258\n",
            "Epoch 19/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 28493.6289\n",
            "Epoch 20/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 28210.2812\n",
            "Epoch 21/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 24756.1797\n",
            "Epoch 22/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 30894.9023\n",
            "Epoch 23/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 27976.7930\n",
            "Epoch 24/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 27483.5391\n",
            "Epoch 25/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 24682.0039\n",
            "Epoch 26/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 27871.8164\n",
            "Epoch 27/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 25550.3340\n",
            "Epoch 28/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 32351.6895\n",
            "Epoch 29/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 22526.7559\n",
            "Epoch 30/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 26839.4570\n",
            "Epoch 31/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 30631.1445\n",
            "Epoch 32/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 21546.9062\n",
            "Epoch 33/100\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 27178.1934\n",
            "Epoch 34/100\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 28842.3594\n",
            "Epoch 35/100\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 25000.9922\n",
            "Epoch 36/100\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 28022.5781\n",
            "Epoch 37/100\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 19372.5859\n",
            "Epoch 38/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 27184.0762\n",
            "Epoch 39/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 23650.1543\n",
            "Epoch 40/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 27107.8066\n",
            "Epoch 41/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 26113.2637\n",
            "Epoch 42/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 26325.3301\n",
            "Epoch 43/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 25383.8047\n",
            "Epoch 44/100\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 22864.2227\n",
            "Epoch 45/100\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 26123.1387\n",
            "Epoch 46/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 20961.7539\n",
            "Epoch 47/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 28743.7422\n",
            "Epoch 48/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 24556.4258\n",
            "Epoch 49/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 22240.2441\n",
            "Epoch 50/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 24633.8672\n",
            "Epoch 51/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 26377.5547\n",
            "Epoch 52/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 30417.1797\n",
            "Epoch 53/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 21912.3184\n",
            "Epoch 54/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 22890.3516\n",
            "Epoch 55/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 26078.3691\n",
            "Epoch 56/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 24382.1934\n",
            "Epoch 57/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 23178.9922\n",
            "Epoch 58/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 25745.1035\n",
            "Epoch 59/100\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 23616.4551\n",
            "Epoch 60/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 21792.9590\n",
            "Epoch 61/100\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 23748.6387\n",
            "Epoch 62/100\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 21231.4375\n",
            "Epoch 63/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 21877.2363\n",
            "Epoch 64/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 27654.8027\n",
            "Epoch 65/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 19763.1133\n",
            "Epoch 66/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 19560.8301\n",
            "Epoch 67/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 23491.3906\n",
            "Epoch 68/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 22073.3516\n",
            "Epoch 69/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 20100.1621\n",
            "Epoch 70/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 19107.7109\n",
            "Epoch 71/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 24103.8555\n",
            "Epoch 72/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 19139.8125\n",
            "Epoch 73/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 24958.9980\n",
            "Epoch 74/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 27317.7305\n",
            "Epoch 75/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 21467.0742\n",
            "Epoch 76/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 19490.6973\n",
            "Epoch 77/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 18615.1699\n",
            "Epoch 78/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 19612.0254\n",
            "Epoch 79/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 21001.3828\n",
            "Epoch 80/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 20490.3809\n",
            "Epoch 81/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 25480.8301\n",
            "Epoch 82/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 23033.8008\n",
            "Epoch 83/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 18842.5566\n",
            "Epoch 84/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 18867.6621\n",
            "Epoch 85/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 21442.6250\n",
            "Epoch 86/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 22010.6816\n",
            "Epoch 87/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 21168.3516\n",
            "Epoch 88/100\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 18411.4609\n",
            "Epoch 89/100\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 21415.1777\n",
            "Epoch 90/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 27305.1934\n",
            "Epoch 91/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 22660.3516\n",
            "Epoch 92/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 17648.7910\n",
            "Epoch 93/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 21796.4746\n",
            "Epoch 94/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 18605.7168\n",
            "Epoch 95/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 20214.4844\n",
            "Epoch 96/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 20273.4004\n",
            "Epoch 97/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 20315.1914\n",
            "Epoch 98/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 19696.3359\n",
            "Epoch 99/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 21012.1602\n",
            "Epoch 100/100\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 20044.6074\n",
            "8/8 [==============================] - 0s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "model.fit(X_scaled_train,y_train, epochs=100, verbose=True)\n",
        "predictions=model.predict(X_scaled_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwsNNXoUiBzu",
        "outputId": "28263096-edc7-49c2-8b54-b2eeeda9a981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE:  102859.078\n",
            "MAE:  267.266\n",
            "R^2:  0.911\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "print(f'MSE: {mean_squared_error(y_test,predictions): .3f}')\n",
        "print (f'MAE: {mean_absolute_error(y_test, predictions): .3f}')\n",
        "print (f'R^2: {r2_score (y_test, predictions) : .3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SHFo6zx-iY_n"
      },
      "outputs": [],
      "source": [
        "from tensorboard.plugins.hparams import api as hp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Of2Fx1-LiZ3T"
      },
      "outputs": [],
      "source": [
        "HP_HIDDEN = hp.HParam('hidden_size', hp.Discrete([64, 32, 16]))\n",
        "HP_EPOCHS = hp.HParam('epochs', hp.Discrete ([300, 1000]))\n",
        "HP_LEARNING_RATE = hp.HParam ('learning_rate',hp.RealInterval(0.01, 0.4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "sMOhnAgcic8d"
      },
      "outputs": [],
      "source": [
        "def train_test_model(hparams, logdir):\n",
        "    model = Sequential ([\n",
        "        Dense(units=hparams[HP_HIDDEN], activation='relu'),Dense(units=1)])\n",
        "    model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),\n",
        "                    metrics=['mean_squared _error'])\n",
        "    model.fit(X_scaled_train, y_train,\n",
        "                validation_data=(X_scaled_test, y_test),\n",
        "              epochs=hparams [HP_EPOCHS], \n",
        "              verbose=False, \n",
        "              callbacks=[tf.keras.callbacks.TensorBoard(logdir),\n",
        "                         hp.KerasCallback(logdir, hparams),\n",
        "                         tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0,patience=200,verbose=0,mode='auto')])\n",
        "    _,mse = model.evaluate(X_scaled_test, y_test)\n",
        "    pred = model.predict(X_scaled_test)\n",
        "    r2 = r2_score (y_test, pred)\n",
        "    return mse, r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "FEo_c3COigg1"
      },
      "outputs": [],
      "source": [
        "def run (harams, logdir):\n",
        "    with tf.summary.create_file_writer(logdir).as_default() :\n",
        "        hp.hparams_config(hparams=[HP_HIDDEN, HP_EPOCHS, HP_LEARNING_RATE], \n",
        "                          metrics=[hp.Metric('mean_squared_error',display_name= 'mse'),\n",
        "                                             hp.Metric('r2', display_name='r2')])\n",
        "    mse, r2 = train_test_model(hparams, logdir)\n",
        "    tf.summary.scalar('mean_squared _error',mse, step=1)\n",
        "    tf.summary.scalar('r2', r2, step=1)\n",
        "    for hidden in HP_HIDDEN.domain.values:\n",
        "        for epochs in HP_EPOCHS.domain.values:\n",
        "            for learning_rate in tf.linspace(HP_LEARNING_RATE.domain.min_value,\n",
        "                                         HP_LEARNING_RATE.domain.max_value, 5):\n",
        "                hparams = {\n",
        "                    HP_HIDDEN: hidden,\n",
        "                    HP_EPOCHS: epochs,\n",
        "                    HP_LEARNING_RATE:float ('%.2f'%float(learning_rate))}\n",
        "                run_name = 'run-%d' % session_num\n",
        "                print(' --- Starting trial: %s' % run_name)\n",
        "                print({h.name: hparams[h] for h in hparams})\n",
        "                run(hparams,'logs/hparam_tuning/' + run_name)\n",
        "                session_num += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=/content/logs/hparam_tuning --port=6006!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wkk0XiSCSl2",
        "outputId": "48b2f7c7-1a4f-4c4f-dec4-da4a2034c418"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-17 02:25:41.821350: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-17 02:25:41.821469: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-17 02:25:41.821487: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "usage: tensorboard\n",
            "       [-h]\n",
            "       [--helpfull]\n",
            "       [--logdir PATH]\n",
            "       [--logdir_spec PATH_SPEC]\n",
            "       [--host ADDR]\n",
            "       [--bind_all]\n",
            "       [--port PORT]\n",
            "       [--reuse_port BOOL]\n",
            "       [--load_fast {false,auto,true}]\n",
            "       [--extra_data_server_flags EXTRA_DATA_SERVER_FLAGS]\n",
            "       [--grpc_creds_type {local,ssl,ssl_dev}]\n",
            "       [--grpc_data_provider PORT]\n",
            "       [--purge_orphaned_data BOOL]\n",
            "       [--db URI]\n",
            "       [--db_import]\n",
            "       [--inspect]\n",
            "       [--version_tb]\n",
            "       [--tag TAG]\n",
            "       [--event_file PATH]\n",
            "       [--path_prefix PATH]\n",
            "       [--window_title TEXT]\n",
            "       [--max_reload_threads COUNT]\n",
            "       [--reload_interval SECONDS]\n",
            "       [--reload_task TYPE]\n",
            "       [--reload_multifile BOOL]\n",
            "       [--reload_multifile_inactive_secs SECONDS]\n",
            "       [--generic_data TYPE]\n",
            "       [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
            "       [--detect_file_replacement BOOL]\n",
            "       [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
            "       [--whatif-data-dir PATH]\n",
            "       {serve,dev}\n",
            "       ...\n",
            "tensorboard: error: argument --port: invalid <lambda> value: '6006!'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPQ9e5A-ij1O",
        "outputId": "1cf7830a-3235-4efc-ef57-81345a9e9145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "model = Sequential ([Dense(units=16, activation='relu'),Dense(units=1)])\n",
        "model.compile(loss= 'mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.21))\n",
        "model.fit(X_scaled_train, y_train, epochs=1000, verbose=False)\n",
        "predictions = model.predict(X_scaled_test)[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "OX-XojcRvvwj",
        "outputId": "97c6d724-91be-4f84-b76e-c37dcec80556"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAE0CAYAAAAL2cVOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAACACElEQVR4nO2dd3gVxdeA30nvlQRCEhICoYQOoUjvTRTsgoqgiF2xy++zK/aKHUWxoKKiiA1RBEEQ6UiHQIAkBBLSe7vz/TGbcBPSk5t7IfM+zz737uyZ2TO7s3t22hkhpUSj0Wg0moZiZ20FNBqNRnN+oA2KRqPRaBoFbVA0Go1G0yhog6LRaDSaRkEbFI1Go9E0CtqgaDQajaZRcLC2Ak1NixYtZHh4uLXV0Gg0mnOKrVu3npZSBlQn0+wMSnh4OFu2bLG2GhqNRnNOIYQ4VpOMbvLSaDQaTaOgDYpGo9FoGgVtUDQajUbTKDS7PpTKKCoqIj4+nvz8fGurotEA4OLiQkhICI6OjtZWRaOpNdqgAPHx8Xh6ehIeHo4QwtrqaJo5UkpSUlKIj4+nbdu21lZHo6k1uskLyM/Px9/fXxsTjU0ghMDf31/XmDXnHNqgGGhjorEldHnUnItog2IDpKSk0LNnT3r27EmrVq0IDg4u2y8sLKw2bnp6Ou+8807Z/po1a5g0aZKlVdZoNDbOoUOHuOyyy/jiiy+a7Jy6D8UG8Pf3Z8eOHQA88cQTeHh4cP/995cdLy4uxsGh8ltValBuu+22plBVo9GcA5w8eZI+ffqQlZVFWloa06ZNa5LzaoNio8yYMQMXFxe2b9/OoEGD8PLyKmdounbtyk8//cTDDz/M4cOH6dmzJ2PGjOHCCy8kOzubyy+/nN27d9OnTx8+//xz3YSi0TQjvv76a7Kyshg6dCjbtm1DStkk7wBtUGyY+Ph4NmzYgL29PU888USlMs8//zy7d+8uq+GsWbOG7du3s2fPHlq3bs2gQYNYv349gwcPbjrFNRqNVVi3bh3ffPMN//zzDz169ODaa69l7dq1/PXXX+Tn5zNu3DiLGhZtUCowZ86cspdzY9GzZ09ef/31Ose74oorsLe3r3O8fv36ERISUnbuo0ePaoOi0TQDHn74YTZs2ADAc889R58+fQC44YYbiI+P5+jRo7Ru3dpi59cGxYZxd3cv++/g4IDJZCrbr25IqbOzc9l/e3t7iouLLaOgRqOxGQ4cOMCGDRsYO3YsiYmJXHvttVAEvdpGkpqZw/Tp0y1qTEAblLOoT02iKQgPD+enn34CYNu2bcTGxgLg6elJVlaWNVXTaDQ2wKJFi7C3t2fRokUEBQVhKjHR5U539l+fT7skRx6a85DFddDDhs8RLrvsMlJTU+nSpQtvvfUWHTp0ANQIsUGDBtG1a1ceeOABK2up0WisQU5ODh988AEXXnghQUFBAHy/6Ev2t8ynTaoDRwKK8PXwtbgeuoZiY1TV+e7q6srKlSsrPVZxnPnw4cPL/r/11luNpZpGo7FRPvroI1JSUnjwwQfLwn5Y9z52YXBF0VW8Ihbz50+/cOVN0y2qh66haDQazTmMlJJXX32VwYMHM2jQIH5dspy+s3z5w28DPU94c+H46wHY9t9qi+uiDYpGo9Gcw+zdu5ejR49y1aVXkp2Rzau/3MmW0HQSvUsYJEYxaOww3ArhcMZOi+uiDYpGo9Gcw/zxxx8AfLvjOSKf8mZ1+HGuPNyHr4IW8eLri3FycSIixZ1jTkctrovuQ9FoNJpzkIKCAnbt2sXvv/9OZGQkB1sc5qSXCXsT3DfjTfqNvKBMNiwnmH+DDllcJ11D0Wg0mnOM5ORkRo4cSd++ffn5558ZPmg4iV4mLovpwedBH5UzJgBtXbuQ7yA5uj/Wonppg6LRaDTnGHPmzGHr1q3MnDkTe3t72geHAtA7eAxX3zLzLPnnXvqUtCeLCO9k2QXbtEGxEYQQ3HfffWX7L7/8cpVDiBuT4cOHs2XLlgans2PHDn755ZdG0Kg81nDH/8QTT/Dyyy8D8Nhjj5W1UVdGxXwvX76c559/3uI6apovmZmZfPfdd9xwww189NFH5OTkkF9wGoBu3QZUGsfD2wMHR8v3cGiDYiM4Ozvz3Xffcfr06UZNV0pZzmWLpbCEQWlMlzH1vQ5PPfUUo0ePrvJ4xXxffPHFPPzww/XSUdO8OHjwICNHjmTnzrqNvlq6dCn5+flMn67mlDg7O3P09G4ABo0Z0eh61gVtUGwEBwcHZs+ezWuvvXbWseTkZC677DL69u1L3759Wb9+PVD+SxqUS/ujR49y9OhROnbsyPTp0+natStxcXHceuutREdH06VLFx5//PEa9QkPD+fxxx+nd+/edOvWjf379wNqRu4NN9xAv3796NWrFz/88AOFhYU89thjLFmyhJ49e7JkyRK6detGeno6Ukr8/f359NNPAZg+fTq///47+fn5zJw5k27dutGrVy9Wr1Zj5BctWsTFF1/MyJEjGTVqVDmdNm/eTK9evTh8+HC58EWLFjF58mSGDx9OZGQkTz75JECl1+Gll16ib9++dO/evdx1mDdvHh06dGDw4MEcOHCgLHzGjBl8++23ZecfOHAgPXr0oF+/fmRkZJyV70WLFnHHHXeUnX/kyJF0796dUaNGcfz48bI077rrLgYOHEhERERZ+prmQ0pKChdeeCGrV68ut0BedZhMJiZPnszdd99NZGQk/fv3LzuWUBJLcLo9foF+llK5dkgpLbIBocBqYC+wB7jbCO8JbAR2AFuAfka4AOYDMcB/QG+ztK4HDhnb9WbhfYBdRpz5gKhJrz59+siK7N2796ywpsbd3V1mZGTIsLAwmZ6eLl966SX5+OOPSymlnDp1qly3bp2UUspjx47JTp06SSmlfPzxx+VLL71UlkaXLl1kbGysjI2NlUII+c8//5QdS0lJkVJKWVxcLIcNGyZ37twppZRy2LBhcvPmzWfpExYWJufPny+llPLtt9+WN954o5RSyrlz58rPPvtMSillWlqajIyMlNnZ2fLjjz+Wt99+e1n8m2++Wf70009y165dMjo6Ws6aNUtKKWX79u1ldna2fPnll+XMmTOllFLu27dPhoaGyry8PPnxxx/L4ODgMn1Xr14tL7zwQrl+/XrZu3dveezYsbN0/fjjj2WrVq3k6dOnZW5uruzSpYvcvHnzWdfht99+kzfddJM0mUyypKREXnjhhfKvv/6SW7ZskV27dpU5OTkyIyNDtmvXruy6Xn/99fKbb76RBQUFsm3btnLTpk1SSikzMjJkUVHRWfk23580aZJctGiRlFLKhQsXysmTJ5elefnll8uSkhK5Z88e2a5du0pKhG2US41luPLKK6Wjo6Ps3bu3DAwMlMXFxTXGiY2NlYAcOXKk/OOPP8rCf/jsG9npVhfZ7wY/S6osgS2yhverJRvVioH7pJTbhBCewFYhxO/Ai8CTUspfhRATjf3hwAQg0tj6A+8C/YUQfsDjQDQgjXSWSynTDJmbgH+BX4DxwK8NUXrOHGhk7/X07Am18Tnp5eXF9OnTmT9/Pq6urmXhf/zxB3v37i3bz8zMJDs7u9q0wsLCGDDgTHvq119/zYIFCyguLiYxMZG9e/fSvXv3atO49NJLAejTpw/fffcdACtXrmT58uVlNaP8/PyyL29zhgwZwtq1awkLC+PWW29lwYIFJCQk4Ovri7u7O3///Td33nknAJ06dSIsLIyDBw8CMGbMGPz8znxp7du3j9mzZ7Ny5coqvaWOGTMGf3//Mr3//vtvpkyZUu46rFy5kpUrV9KrVy8AsrOzOXToEFlZWVxyySW4ubkBqtmqIgcOHCAoKIi+ffsC6l7VxD///FN23a677rpybjGmTJmCnZ0dUVFRnDp1qsa0NOcPP/zwA19//TXz5s2jffv2XHXVVfz9998MGzas2nh79uwBVDPsoEGDAHj83tt4yvtdaAlTDkdaXPeasJhBkVImAonG/ywhxD4gGGUUSp9Gb+CE8X8y8KlhCTcKIXyEEEEoY/O7lDIVwDBK44UQawAvKeVGI/xTYAoNNCjWZs6cOfTu3ZuZM8+M1DCZTGzcuBEXF5dystW5tDd3fR8bG8vLL7/M5s2b8fX1ZcaMGdW6vy+l1A2+uQt8KSVLly6lY8eO5WT//fffcvtDhw7l7bff5vjx48ybN4/vv/+eb7/9liFDhtR4XnPdAYKCgsjPz2f79u1VGpSKiwaV7punJaVk7ty53HzzzeVkreFh2nyJAVXkNecby5YtIzU1lRtuuKFc+OLFiwkJCeGBBx6goKAAZ2dnfvrppxoNSulHZVRUFAB//bSKV13epWeCJyPtJ3HVjDstk5E60CR9KEKIcKAXqiYxB3hJCBEHvAzMNcSCgTizaPFGWHXh8ZWEN4jXX4c1axp3q8v7ys/PjyuvvJKFCxeWhY0dO5Y333yzbL90AbDw8HC2bdsGlHdpX5HMzEzc3d3x9vbm1KlT/Ppr/W3uuHHjePPNN8tegtu3bwfOdqMfGhrK6dOnOXToEBEREQwePJiXX36ZoUOHAqoGs3jxYkB1Th4/fvwsI1WKj48PP//8M3PnzmXNmjWVyvz++++kpqaSl5fHsmXLyr7gKur+0UcfldXuEhISSEpKYujQoSxbtoy8vDyysrL48ccfz4rbsWNHEhMT2bx5MwBZWVkUFxdXu3zAwIED+eqrrwD1EqmNMdWcH7z66qtccskl3HzzzWcNtNm+fTv9+vXD0dERDw8PNSnRqJ2bc+211/LSSy+V7e/Zs4egoCB8fX0xlZi4b9ll2En48NpVvPLuF2fNPbEGFjcoQggPYCkwR0qZCdwK3COlDAXuARZWF7+RdJgthNgihNiSnJxs6dM1mPvuu69cIZw/fz5btmyhe/fuREVF8d577wFVu7SvSI8ePejVqxedOnVi2rRplb5sa8ujjz5KUVER3bt3p0uXLjz66KMAjBgxgr1795Z1TgP079+/TKchQ4aQkJBQtnLkbbfdhslkolu3blx11VUsWrSo3Fd7RVq2bMlPP/3E7bffflZtCNQqlZdddhndu3fnsssuIzo6+iyZsWPHMm3aNC644AK6devG5ZdfTlZWFr179+aqq66iR48eTJgwoaxZyxwnJyeWLFnCnXfeSY8ePRgzZgz5+fmV5ruUN998k48//pju3bvz2Wef8cYbb9TyKmvOdZ5//nm6detGcXEx33zzTVl4ZmYmMTExZc2uxUXFREREcOTIkXLxpZQsW7aMV199lZKSEkDVUEprJ/+bcyNbQzOYnXkdfYaeXV6tRk2dLA3ZAEfgN+Bes7AMjM5zVEd8pvH/fWCqmdwBIAiYCrxvFv6+ERYE7DcLLydX1WarnfKa+lOxY/x8QZfLc5OkpCQJyFdffVV26dJFDhw4sOzY2rVrJSB/+uknufzzb6XnXOTwK1vLYL8gaTKZyuTS0tIkqntArlq1SppMJunu7i7vuusuuXvzLunzkJC9Z3nJosKiJssXteiUt1gNRahG7IXAPinlq2aHTgCljYUjUSO3AJYD04ViAJAhVT/Mb8BYIYSvEMIXGAv8ZhzLFEIMMM41HfjBUvnRaDSa2rBv3z4AunTpwjXXXMOGDRs4duwYcKaJuEf3Hjy1ehYS+LvjCdJuSeSBW8+sVZKQkFD2//PPPyc2NpacnByioqK4f/4Uch0lz4//skkmK9YFSzZ5DQKuA0YKIXYY20TUqKxXhBA7gWeB2Yb8L8AR1BDgD4DbAKTqjH8a2GxsTxlhGDIfGnEOc453yGvqx4wZM/RCYhqbwbzzvE9UTwb1bM+336i5Rtu3bycwMJDvPv6YLaHpzE6byov2L9A63Ymf7M7MR4qPV93D3bp14+OPP2bQoEHY29szZMgQtrQ8wshjEYy5bGLTZ64GLDnK629Uk1Zl9KlEXgK3V5HWR8BHlYRvAbo2QE2NRqNpVPbu3YunpyfBwcFc90h/1k85gds3P3Lvffeyfv16+vTpw98xP+AcBo/Pe48TSSf47qFXiWl/pn+31KB8++23vPPOOyxbtky5AMqTnPaQRKR3s1b2qkXPlDeQeuimxobQ5fHcpbTzfNHrb7OmrZoVkS8T+O233zh06BCTJ0/mgOsBok564eXnRXh4OPa5TqS6mTCVqGkApU1egf6BPPLQIxw9epThw4fz96rfAIhqW7nPLmujDQrg4uJCSkqKfog1NoGUkpSUlLPmHWlsGyklGzduZNeuXURFRfHZjudpkS0IyrCnsG0q06ZNw87OjpHDRrKvZTYdCzoB6v3jUOBKoQOcjDsJqBpKYGAg0+dcwMB5IRQXqXlgew7/A8CQkeOtk8kasK0eHSsREhJCfHw858KQYk3zwMXFhZCQEGuroakDS5cu5YorrgCgW9du/HziE3qfCsMOezaFHyEtXTJ61Gg2/PYHRfbQu80ZX3XOwhOA2AMxtA5vTXx8PCEhIez12c3hFoW88/zz3PXoIxzN3YNvrqBr3+q9XFgLbVAAR0dH2ra17DoBGo3m/Oa9996jTZs2LFy4EMcCQVKWiaic/rg5ebLC7TCXj7mIWffezvdfz4c2cOEl15TF9XBRboPiYg8DQ4mPjyc8OJzt/mri8jdH3uQuHuG4azxtUz2xs7fNxiXb1Eqj0WjOEZKSkvj1119ZtWoVs2bNYvTo0fy5SvlwG9JvMlOvuROHEqBlPGNGj2Gd02o6JrkQ1adLWRr+/kEAJJxQExwTEhLwdXJGCog66cr6sCQeuuN6jvhnEZof2uR5rC3aoGg0Gk0dKSoq4pZbbqFbt260bNmSiRMn4uDgUOaDb1fyOtwLYeJVl9C1X3dGHA1jZfB2Frz0Cntb5THRNKVceqHh7QBISDzCkSNHSE1NpVAobxmvjvyabonuvBjwKQUOMKHr2Ssy2graoGg0Gk0d2bp1K++//z6+vr4888wz/Pjjj2zevJmQENWB/p/PATqf8sHFTQ2smDHgUTJd4MHMB/EogPsfeqVcel379ADgRMoxoqOj8fb2JsstEe88wZhLJ/Ll7H+54nAfvu/6Azc/dN9Z+tgKug9Fo9Fo6kjpstlffPHFWYMnHr57BodbFvJYxqyysGm33cg/s39mS/Fa+joMo3V4ea/Z0Rf0xXEnJOckkJaWxt69e7n69b5EnvbGzt6OqD5d+PrThi/VbWm0QdFoNJo6snXrVgIDAwkOLu/gPCkhiY88v6BPnDePv/9muWNvLviuyvRaBbXCL9eOQuc8OnfuzKGte/ivdQ4zj460iP6WQjd5aTQaTR3ZunUrffr0OWsdng/eeJ40N8nsDo/VaSSWEAKvXAcK3QoZPnw4r/0+B698eOyhDxtbdYuiDYpGo9HUgdzcXPbs2UOfPmc8SB2POc7bzz7PqlNfE5Btx4y77qhzuh55zuS7FdAhuB1rIhKYfGIw4Z3OrekMuslLo9FoasmaNWt4++23MZlMREdHM2PGcLq1GcKqo1/xa7sYRFu4+HAXnFyc6py2R74b8S1SiYvbDkFwxdg5jZ8BC6MNikaj0dSCTz75hBtvvBEPDw+6du1KkG8rPg3/C//ctWS3kQRl2HPSq4TL+tVvKV4fOz/2uCXx38l1BGbZceHVlzRyDiyPNigajUZTAyaTiYcffpj+/fuzYsUKPD09eeqBu5EecNpd+QB8v88SOnbrSofulS9lXROtXdqS6raPDSHHGRIXYbOz4avj3NNYo9FompiNGzdy8uRJ7rjjDjw9ld+tTSdX4J0nuDCmA8OPBHPRNZfV25gAPPXUx0QmO5PrBNH+YxpL9SZFGxSN5hyhsLBQOzC1Et999x2Ojo5MnKgWtTKVmNgecJheJ4P46bMDrP4kvsHnCAwO5N3RPzDmcBg33Tq3welZA21QNJpzgIMHDxIdHU1ISAjPPffcWUstHD16lBEjRhAXF2clDc9vvv/+e0aPHo23tzcALz46lxPeJfRxG9Go5xk1ZRwrPz1KWIewRk23qdAGRaM5B7jppptISEhg1KhR/O9//+Pvv/8ud/zHH39kzZo1vPTSS1bS8PwlOzubI0eOMGTIEPJz85n30H08b3qJLifdeOyZd6ytnk2hDYpGY+NIKdm5cydTp05lyZIlODs78/3335eTKXUF8uGHH3L69GlrqHnecuzYMQDCw8O56dbRPOL2Kl4F9rx38c94+XlZWTvbQhsUjcbGOXXqFBkZGXTs2BFPT09Gjx7N999/X67Za/PmzXTq1Im8vDw++eQTK2p7/lFqUIJbB7PKfyMXHGtBzHM5DJ4w3LqK2SDaoGg0Ns7+/fsB6NRJLRl7ySWXcPToUa688kpWrVpFVlYW+/fv5+qrr6Zr16788ssvgHIP8sgjj9hsjeXo0aN8/PHHNr/0dqlB2b1hE4neJYz2vrxeExebA9qgaDQ2zoEDB4AzBmXKlClERkby66+/csMNN/DPP/8gpaRv376MHz+edevWsWjRIgYNGsS8efPo3LmzzXXWl/ZJ3HDDDezcudPa6lRKZmYm3377LceOHcPR0ZHVexfjVAyzbj83R2A1BdqgaDQ2zv79+3FzcyOoVRB7tuwm41Q6Bw8eZNmyZRw/fpxZs5Sb9L59+zJhwgSKioqYOXMmvXv3ZunSpZw+ffqsTnxrkpGRwcSJE8nKygJg9erVABQUFFBQUGBN1crxyiuvcMUVV/Djjz/Spk0b9ngcoNcJP9q0b2Nt1WwWbVA0Ghtn//79dOzYkVtunkjXn7vR/uv2jJkeTmSbSCZMmEB2djavv/46/n7+bFjxEyH+rfH19eWbb75hzBg1QS4+vuHzJBqLW2+9lcOHD7N8+XIiIyP5888/OXLkCJ06dWLo0KEUFhZaW0VAzT0B2Lt3L6GhoRz3yyOkSBuT6tCuVzQaG+fAgQMMGDCAPaykTZoDF6T25OuILTz67HX88MMqpJQ4OTkx/+lneNT9NcKnOfJWt4Vla3V4e3vbTJOXlJKff/6ZmTNnMnToUEaOHMnixYsZOnQoGRkZHD16lKeeeopnnnnGajqeOHGC2NhYdu/eXRYW6t+aNU4Q4hZpNb3OBSxWQxFChAohVgsh9goh9ggh7jbClwghdhjbUSHEDrM4c4UQMUKIA0KIcWbh442wGCHEw2bhbYUQ/xrhS4QQuqdMc16Rl5fH0aNH6dixI0d904hKbcNXn26mX5wfa3z/Yc/mXez4eyumEhMLTjxHyyw7kj2LeG/9g2VphISEcPz4cSvm4gyJiYlkZmbSo4da8nbkyJFkZ2dTVFTEhg0bmDp1Kq+88opVm75GjRrF4MGDAZgwYQIALnZq4EBkaC+r6XUuYMkmr2LgPillFDAAuF0IESWlvEpK2VNK2RNYCnwHIISIAq4GugDjgXeEEPZCCHvgbWACEAVMNWQBXgBek1K2B9KAGy2YH42myTl06BBSSgI8AzjpZSLcqTMA47yvIs6nmEE/92H8b4OYNrM/e1rlckPhTKbED2R92Ek2/fkPqUmphLUIZXXnH/i/u2+ycm5g3759AHTurPIxadIk7r//ftauXUu3bt24/PLLyc/PZ9u2bY16XpPJVDaarDpjdfjwYfbv30+vXr2YNWsWN9xwAwCFpALQu9+QRtXrfMNiBkVKmSil3Gb8zwL2AWXrZQq11NmVwJdG0GTgKyllgZQyFogB+hlbjJTyiJSyEPgKmGzEHwl8a8T/BJhiqfxoNNagdMhwRlICAFFhFwBw+31P4J0n8M9xQApY0m4LI4+E8MSL73D7tS8gBUz5eTCt3vLnv56/k+kCf2f8ZLV8lGJuUJ564G46/Z83rsXZdOyonCr6u/nRPSyCDRs2NOp5Z8yYQdu2bZk8eTJeXl4cPXq0UrnffvsNgCVLlvDBBx8wYcIEZl4xnVyn0zgVQ58h/RpVr/ONJumUF0KEA72Af82ChwCnpJSHjP1gwLyhN94IqyrcH0iXUhZXCK/s/LOFEFuEEFu0cz2NrZKZmXnWnIzSIcMnUtSLeMgo1QQTGBzIr6PWsuGuWOYHv8vUw/35/rU9OLk4ccGYwQw52opUNxP941qS5FkCQLEoacLcVM7evXvx9vYmMymDN+zeJMmjmKd93mP+06rP5MFvLuW/mUdY//PPZXFOnTpFQkJCg867du1ajh8/zo8//khhYWGVNaCVK1cSHh5O+/btAfjw5Vf4uMun/BS+ldB0Jz3/pAYsblCEEB6opq05UspMs0NTOVM7sShSygVSymgpZXRAQEBTnFKjqRPZ2dmEhIRwxx13lDMq+/fvJywsjKMF+/DLFXTv37Ps2AVjBhMSEcJ1d97MF59uLOcG5MsntrLpwu2s+/gkr7V8m377fTjtZv74WYd9+/bRuXNnHn/lOjKdJYvDP8crH1Ye/IwlCz5hU5s03Arhz36ruee2ezh16hTTp09nyJAhFBcX13yCSsjNzeXYsWM8+uijxMTEAJT9mlNUVMSff/7JuHHjEEJgKjHxQeILAOQ5QnCWX/0z3kywqEERQjiijMliKeV3ZuEOwKXAEjPxBCDUbD/ECKsqPAXwMdIyD9dozjm2bdtGVlYW77zzDh988EFZ+P79++nUqRPHXOOISPGu9aJLrcNb031ATwDatmuLS4YbJ72sP8dj3759REVFcdgphi4nPbjsxmvoeyKUTa1imL/xAbzzBNOPXkWGK6xd+RPvvfcef//9N7GxsXz77bdIKfnss8/q5Mb/0CHVCNKlSxciIiIIDAwsCzNn+/btZGVlMXLkSEwlJm6/+WL2BOUy9XB/HEugjWzXaNfhfMWSo7wEsBDYJ6V8tcLh0cB+KaX54PjlwNVCCGchRFsgEtgEbAYijRFdTqiO++VSfcatBi434l8P/GCp/Gg0lqTUuWPXrl157bXXADXE9sCBA0RGRHIgMId2he3rlXZISAhkOJPpAsdjrDfaa9++fZw6dYquXbuS6JlFq/xAAAb5TyLZw8SGsGSmpVzEff97GgD/Nva8//775ObmYm9vz4svvsiGDRuYPn16nfyVVXRdExkZWalBWbduHQBDhgxh6sx+vBf6M4OPtuTDt//k5x4/88KTXzco/80BS9ZQBgHXASPNhglPNI5dTYXmLinlHuBrYC+wArhdSlli9JHcAfyG6tj/2pAFeAi4VwgRg+pTWWjB/Gg0FmPLli20adOGm2++mf3797N//37i4+PJycnBpQQKHaBbq/qNMAoJCaEoU1Xkd23e2phq14lnnnkGNzc3Jk2YxAnvEoLs1Zof18+6D8cS6Bvny/x3ltK+SySh6Q7kt0yn5dgshvRtx//93/+xffv2Mq8AdZmoeeDAAYQQREaqOSTt27evtMlr3bp1tG/fnuWffsHX7bZycUwUqxfE4+bpxpjLJtI6vHUjXIXzG4tNbJRS/g2IKo7NqCJ8HjCvkvBfgF8qCT+CGgWm0ZzTbNmyhejoaKZMmcKdd97JsmXLsLe3ByArX9UqRo65tF5p+/j4kJ+jvh1jDu4CLmkUnevC5s2b+eqrr7jvvvs4susAUkAbXzV0OCKqHZ+HfsKA6cNxcFSvpLZpAWxul0ieI3SzN5F3OpaW99rh8n0RUHuDkp6eXtYP5erqSszuQyRl/YPP+FTm3jaHS2deQ1ZWFh999BFr165lypQpvHn0McKdHFn44l9l+mhqh75aGo2VSU9P59ChQ8ycOZOQkBD69u3LwoULycrKYsyYMcTJfbTIEfQfObBe6QshcPZQHfZxpw40puq1Yt++fYwfP57Q0FAeeOABPn1zPthD5w7RZTJX3jS9XJwIOrLWMRGAA61zMSUv5ZSXiZRrD9P10061GvV1/PhxIiMjKS4uZsyYMRw7eIzJb/fgUJc8TAL8//2WN0d8iIODAxkZGQD07dWXT09/zLRjQ2gR1KIRr0LzQPvy0mgsSFFRUY3u2UvnPvTt2xeAp556itOnT3Pq1Cnmzp3LIa/jdExuUesO+cpoGd4aexOczDla7zTqy5133okQgj///JOAgACOJO4CoN/QoVXGiQpR8218cwWFDrAnKJdRR0IptofIjj61Mij79u2jsLAQk8lEr169+N+8qzkQmMfcjLuJincjNSSVli1bYm9vz6+//srdd9+NqwlK7KBr8KDGyXwzQxsUjcZCSCnp2LEjl19+OcePH6+0maa4uJjHH3+cqKgoRowYwerlv9PKK5D9+/ezYsUKvB09iWlRQCd6NEiXtu3a0irTntPyZIPSqSurVq1i1apVPPLII0RERABwIv8wPnmCiKiqR01NuPhq3Avh1vwzzi+uj/4//HIFmd4nKM4sJHb/kWrPXeq/bM2aNTz66KPsdd5Dt0RP5r7wPK1OBXEwKI8/V/7Jnj17GD9+PM8/+zy79quO+aEjJjU0680SbVA0GguRlJREbGws3333HWFhYXTr1u2suRRfffUVBw4c4JlnnuFU3CkuWz+Ocd/15ch/hxg3bhwfLnoSKeCaSx6s4iy1IyIiAv8MZ047pTQonbry7rvvEhQUxC233FIWdtIxkZB0t2rjde3XndRHC5j3xge0T3bGN1dw1U0z6ZTcgiMtEzHNSuaOZ8ZXm0ZcXBxCCAYOHEhhbiF7W2bRMb8TLi4ujOx+GYUOsPaXFbRq1Yrn5z5Iy6dd+dnue3xzBf1GXNAo+W9uaIOi0ViIgwcPAvDggw9yzTXXkJ6ezt69e8vJfPnll7Rt25YpU6bw+FMzSXOT5DqZmPXdWEwlJtY4rqLbCXdGXDymQbq0bdsWjww3kjxyGpROXTl27Bg9evRg61+bGHhDAOt/W8vBgFSCc1vWGLd0Vvpsn3u5s+gWnFyciDRFEduiiFOeJk45J1UbPy4ujqCgIBwdHfnxyyUUOkDv0FEATJpyDQDz9s9h8MxAnhUvkekCBwLziTzt26DmxeaMvmoam+S1117jpptusvnlYaujdK7D7Nmzefzxx4Ez800AsrKyWLVqFVOmTCHucBzfBfzOwGMB3JR2NXtb5fHIPbPZ1yqP4XJcpenXhYiICBwzXTnpVUxxUf1mnNeHxMREgoKCeHLxdP4JO83MH8eS6iYZF3FdrdN44JlnefLVdwDo1XZkWXiaW1618Y4fP05oqJoTvWHnjwBceIkyJF37dafLSTdS3AtIcs/EJ9eBOxLU6LeIovrN99Fog6KxUd555x0+/PBDvvnmG2urUm8OHjyIo6MjYWFhtGvXDm9vbzZv3lx2/LfffqOoqJCuEVHc99QU0lwl9w56jTvufAbHEnjZZyFe+XDvvS83WJfw8HBMGU4U2cPerbtrjtAImEwmTp06hXOB4I+IY3gWwKGAAlpm2XHrgw/XnEAlXHTFNbgXQotswWmP6hfiiouLo02bNjx+72386LmCsFQHukR3LTu+Y34GifMKOfhWPsdfL+KNd7/lvlPTuGfG6/XSTaMNisYGiY+PJyYmBnt7e+bMmVNuBb9FixZZdfGlunDo0CEiIiJwcHDAzs6Ovt2jid25BVOJCVAebYdd0J4bU27i23bbmXS4E5fdeA0RUe0YcLwVRfYwOXEI4Z3aNlgXNzc3yHcFYPe2ppncePr0aYqLizmZvxU7Ca+3mA/AqOR+uLi51CvNiKh27Lk+li4bw8l0gaQE1eyVl1e+tiKlJC4ujpa+gTzj+S4uxfbc2+LxcjIOjg7l5pnY2dvx8juL6TdS95/UF21QNDZDcXExDz74IO+99x4A//vf/0hMTGTjxo0kJSUxb948Zs6cyRNPPEFOTtP2BdSHgwcP0qFDB35dspybbhjHnujV/DZuC/OfeYb9+/ezdOlSMiOTCE63Z8rhbrz60PKyuNdGPUD3Ex488eDHjaaPh6dyjHrkaNPUUBIT1TySVNdThKY5csO9d7Kk9Se88fyPDUo3rEM4dgWqU3/f9l0cOHAALy8v1qxZUyaTmppKXl4exZlpmOzgnqAnuevRRxp0Xk3NaIOisRn++OMPXnrpJebNm4evry9z5szB3t6exYsX06lTJx555BEiIyMpKSkp1xdhi5hMJmJiYoiMjOSetVfxYdhKPPMdcS6G7zcuYNy4cXi6ebInOIMBKd34/tP/aN/1zPKysx+8l53vZ1U7tLaujBivRkUdPrGnBsnGodSgnPJKJyTLF1ATGBtjwqC9dAfgyKF97Nmzh+LiYt59992y46UrVCaXHMDOBJdMm15pOprGRRsUjc3w7bff4urqip2dHcOGDcPH24f+ffuzYMEC0tLS+Ouvv/jnn38A2LBhA1JKrr76apvsZ/nvv//Iz8/Hx9GTA4H53Hx8IgfeyadXnD/HI5NwdXXlmounkO8I/UKrH/7aWNx42yw8CiAxp/r5G43FiRMnQMBxv3yCSkJrjlAHnFw8AYhPPFxmuJYtW0ZKihoWXToH5ahnDJ2SXLUfriZCGxSN1TGZTJw8eZLvv/+eSy+9lJUrV/Lyyy8z9+6ZHBjyD66OLlx00UUMHToUf39/OnbsyD///ENiYiJLlizhjjvuICsry9rZKOPkyZNMnjyZFi1acCpZzQq/ZPytAPRzHsFR/yLeffFN8pzjEBKuvG52k+jl5+9HywwnMpzTmuR8iYmJBHkFkucIbTw6NmraHn6qxnMq/WiZQSksLCz7uNi9ezf2dnbsDcqgc07jnltTNTUaFKG4VgjxmLHfRgihHTJqak1qamq1w3/nzp1LUFAQqampXHHFFYwaNYp27dqxOv9HUtwlUW3CmDt3bpl8r4hOZOfuZtO/mwA1gfDllxs+EqqxWLp0KcePH+enn35ih+lvQtMcGHOpcrR99eV3IyQ89v21/NziL7qf8GiUTvfa4pftRoZXbpOcKzExkTZG81aniL6NmrZf6xY4FUNK/omyocne3t7s2aOa89auXcvArn3Ic4TugXod+KaiNjWUd4ALUCssAmQBb1tMI815xZ9//knr1q154oknzjo2d+5cnn76aV5//XXGjRvHm2++yaRJyuVF/JF4trdWX9JdBgVywQVq5M3j997O931+YPWQWL56X3VYd+/end9//71pMlQLEhIScHBwoF1YO7YFJ9E3JapsotwFYwZzc/yF/B2eRJ6jiedGLW5S3bzzfEjyKaj36oc1kZGRwSeffILJZCIxMRGvAOVwPHpg1X676kNAQAAB2fakklRmUNq0acPx48cpLi5m/fr1tA72AaBL5/6Nem5N1dTG23B/KWVvIcR2ACllmrHQlUZTLQkJCUyZMoWCggLefPNNHnroITV8FTXM8/nnnwfAxcWFDz74oGwSGsDnH8yn2AXcCmGvo2o2unXWJN4L/ZmoRDf2BuWSmLqX8PBwOnfuzPbt25s+g1WQkJBAUFAQn7//NnmOMLzDleWOv/3+clzumEb3DoOZcNXFTaqbb1FLUtyPsm/HXrpFdwfgp59+Ijc3lyuvvLKG2NWTn5/PxRdfzNq1a+nSpQuJiYm4BWTjUUC5pYsbA39/f3xOOJHmmEFRYiLBwcGA6ozfsWMH2dnZ2HmoocR9h+kaSlNRmxpKkRDCHpAAQogAwGRRrTTnBStXriQrK4u33nqLtLQ0Pv/887JjsbGxAFx55ZUsWrSozJjk5+bT9RZ3/s/pJbzyYWxcN3a3Smfb31v4KOhnhsa24t8XThGYZUdxSAY9evTA39+f06dPWyWPlXHixAlat27NmphvcCuE6269vdxxO3s7Xnv3K2bec0eT6+bvGg7AP6tXl4U9++yzzJo1i+zs7Aal/eqrr7J27VoAjhw5QmJiIpneGYSmuza6KxM/Pz88slxIdc0hMTGRVi1bEddqDZ6ykL/++guALKfT+OYK2rRv06jn1lRNbe7yfOB7IFAIMQ/4G3jWolppzgs2btyIj48Pt956K127duWrr74qO3b48GEA7r33Xq666qqy8Kfn3smeoFxGx7bl/pI59A+ZQL4j3PjpSAod4OEJ7+Ph7UG7k34kBqfTs2dPWrRoQVpamsWacepKaQ1lS4v99ElohU8LH2urVEZwSAcADsZsKws7ceIEWVlZ5e5PfdizZw/+/v6AWiXx+PHjnPbOoWWub4PSrQx/f3+ccpxJcy8iKSkJLwd3/gvNIT8qgVWrVtG2bVuSnZNoneHa6OfWVE2NBkVKuRh4EHgOSASmSCltb5ymxubYuHEj/fv3x87OjkGDBrF9+/ayzvlSg9KuXTt+XbKcMdeH0eYeR952XUjnk678+nEMj774Gnc89CjRcT7sCM6ib5xvWRNRO6mcBHYM60CLFi2QUpKW1jSjl2rixIkT+Dp4kOBTQj+3kTVHaEI69uqGkJCQqRbaklKWjZJ6//33AVi/fv1ZM89rQ1xcHFFRUfj5+fHHH39gkiZO+BQRaApqvAwY+Pv7Y5fjTJqrBCSyUA002B+ewepVq+nbty8nPTJpleff6OfWVE1tRnkNABKklG9LKd8CEoQQupdLUynvv/8+v/32G1lZWezevZsBAwYA0KtXL9LT0zl69CigDIqXlxeb/vib67ZOYUtQHO3SAgnJcOX+yBfKmkg8vD344ZE9XBTTifv7v152noFdLwQgMXY/LVqokUSlcxCsSU5ODhkZGQipZvJHdx9tZY3K06JVC0LS7Tllr9ZmSUtLo7CwEF9fX7Zs2cLx48cZMmQIX3zxRZVpHDp0iFWrVp0VHh8fT3BQMH3adWD9+vUEevhT4ABBbhGNng9/f39MuQ5IAX5uvuQWqibPLGdo37I13bt2J8G7mJaENPq5NVVTm075d4HeZvvZlYRpNAA88sgjuLi4sHDhQkwmU9norF69egGwfft22rZty+HDh4mIiOD//pyOvZdg6dCVDJs0qtI0W4e3Zvln+8qFTb1pNk88P5claW/z9OAvAWyiH+XEiRMA5AmlS3WrEloDDw8PAlPcOemRCpyZzd67d29WrVrF5s2bkVJWaZxTU1Pp0EE1m5kPBTeZTMTHx3NBlw78fuFGOiWE4WBvRxIptA3qWmlaDcHf35+iHDWCzMfdnUzTSYShjn9ECf6uPhSbIMSzQ6OfW1M1telDEdKs5EgpTei16DWVkJ+fz+nTp4mPj2fGjBk4OTnRu1dvRlwfwucfvIC9vX3ZaKwjR44QERTOztZZXJgyvEpjUhU+LXyYkjaOTW1S2fePStMWDErp0rTpDifxza1+VUJr4OHhgXuKB3F+eZhKTGUGMMw7gKFjQspc2lTVQT9nzpxKw5OSkigqKiLPKRmAgGAHAloq9yhRXaMrjdMQvL29yTfcuXm4OZPicJKgTHs6J7iQ0CWR/GxlENu16d7o59ZUTW0MyhEhxF1CCEdjuxtoGt8NmnMK8yVuT548yaeffsrujTtYE5HAmyHfMXJIJH/99Rdr1qwhNjYWJ/sMpIBJQ2fV63yP/t8CvPME3x95A7ANg1L6gj7tmlzjqoTWwNPTE5niSrYz7N78X1kNJdFhK2sHxbN7gzLOVRmUFStWlP0vKCgo+1967zOdlUExBWfh7KcGSfQdNrDR82FnZ4c0vmu9vJ057ZJKqywPWu5oR0xgIe8mP4edCQYMta0+rPOd2hiUW4CBQAIQD/QHmsZXhOacovSl8u6777JmzRquuuoqNvz9KwCdTrmwbtB+Du7Yx4gRIygsLCTeew9BmXZMmX5VdclWSUhECBclDmRt20SGjgvmm9+ea7S81JfSGsoJ7yyC8gKsrM3ZeHh4UJClmopiDx4sMyjpbmpAQy5qsIS5N+f8/HyefvppEhISSE5Opm1bNbM/IyOjTKbUd1ay0ZR2qnU6BV7ZtMyys9goN+HsDIBfK3dOemUTWNCCdt0vIDjdnoOB+cyMG0P3AT0tcm5N5dTYdCWlTAKubgJdNI1IdnY2Hh4eTXrO0pfKiBEj6NhR+U/ad+pfHMLgzdHfMXHHRHpM9OTmK9/n919/57PA9xh1vHOD5ij835wPWLo0irUXqBd5ZmomXn5eDc9MHZBS8txzzxEaGsqff/6Jj4cPJ7zTGZMW3qR61AZ3d3cK8lQL9qmT8SQmJuLh4UGSRyYAye1PwNryNZSVK1fy2GOPldUA+3Tojo+wIyMjg8DAQODMvU/wyUVIONKiEKfiFIIy3S2WFzdfd9Vv4plPopeJcSmhfLDwA958Opx/Dv3Iewt/sdi5NZVT5ZMshHjQ+H1TCDG/4lZTwkKIUCHEaiHEXiHEHqOprPTYnUKI/Ub4i2bhc4UQMUKIA0KIcWbh442wGCHEw2bhbYUQ/xrhS/QMfsVff/2Fr68v99xzDyZT081BLa2hhIScGVlz1O4Q7VJcGH3pBC482o3VHWLpFNaREF9XcpxgYtcbG3TOTr0680DurXROUAs27d22q0Hp1Zb8/PyyYcpbtmzh//7v/5g+fTorVqzgmkuuQApo49e5SXSpC46OjhQUqjJxOjVRTQps1YoT3oU4F8Pu0Fz83HzKGZTSpYy//FINfkhssYE90w6ze/N/ZTLx8fH4e/iR4i7pHe+NyQ72ts4jML/hruqrwj/AH988wQkvZcyCvZX7/zsf/T+++HRjucWzNE1DdZ+GpcNqtgBbK9lqohi4T0oZBQwAbhdCRAkhRgCTgR5Syi7AywBCiChUTagLMB54Rwhhb8zSfxuYAEQBUw1ZgBeA16SU7YE0oGFvp/OAwsJCbrvtNhwdHXn99deZN29ek507Pj4eX19f3N3PfJUe9kuhbZZyHX7LRc9SZA9vvvsQfyQtITDLjpl339ng8z756juE7FCdr4f2Nc3iUVdffTUBAQFMnTqVBQsW4OzszA8//MC2bdsIa6Neop07NK5DxMbChGryyshWfrDCAkPJc4ReMf7KELb0L9fkdfDgQQCSk5NxsHdgd8hpCh3g6+WvlMnExcXRMSwMgFEuUwjItqPfcV8emGI5t39Tp07FN8eR3UHpAHTrMshi59LUjioNipTyR+Nl3k1K+UnFraaEpZSJUsptxv8slIEKBm4FnpdSFhjHkowok4GvpJQFUspYIAboZ2wxUsojUspC4CtgshBCACOBb434nwBT6noBzjeWLFnC3r17+eqrr7j00kt58cUXOX36NJs2baJDhw4kJSXVnEg9iYuLIyQkhOMxx/ninYV8+PLrnPQyEeHcDYBxV0yiZ4InP3it4J/QEww51Q0nl8apVDo6qfUxjscdaJT0qmPXrl388MMPDBgwgCVLlvDhhx9y0UUXcfHFF9OrVy/+PrYM1yIYOn5czYlZAelkD8Dm/zawYcMG/NzV4AH/ZOX+xsNHVFpDAejfuQcZrhKPAvgz5F9ys9SEwuPHj+Pjp+7l4AsuIumlEv5dmMroSydYLB9Tp07Fu8CVPEfl8+2iqVdY7Fya2lFt47WUsgRosNkXQoQDvYB/gQ7AEKOp6i8hROlnXDAQZxYt3girKtwfSJdSFlcIb9Zs2LABb29vJk2axDPPPENubi4vvvgiCxYs4NChQ2ULVFmC+Ph4QkNDmfXUUK5JnsVNOfdgZ4KhfS8pk7nEeyaJ3iWEp7pw51WvVJNa3XA31sdITItttDSr4uWXX8bd3Z3ly5eXObicOXMmACfjTvJn6D6GHW9ns4s6Obo54lQMJucCSkpKcPNQNRYv57bYm8Dep/Asg+LlpfqlfFqrx63PurYkeZrY8LvymxUTE4O9l5pd33940zlj9C5U/YRdT/ri5ml7o+qaG7XpDd0hhFguhLhOCHFp6VbbEwghPIClwBwpZSZqIIAfqhnsAeBro7ZhMYQQs4UQW4QQW5KTky15KqvxzTffsHr1arZs2ULv3r2xs7Ojc+fOTJs2jbfeeoulS5cC8OeyFeTn5tfrHPv27WPcuHFVrudeWkPZ6x9PdJwPzxU8yNZxW7lq9vVlMo+99AbJs5PZ/3ZeneeeVEdAaCDOxbDn+DYSEhLIyMigsLCw0dI35+eff+bKK6/Ez8+PBx98kPj4eCZOVOudPPfMHWQ7w9V97rPIuRsDLy8vfPLsKHEtZOfOnTj7KCMRFBFOq0x7inzycOuZwK2zJpGbm0t8fDyXTbmM3td6sqHnf0ScdiIjQb06jh45QGZmJklJSeS7ZeGfIwgMDmy6vJh8AIgq6dFk59RUTW0MiguQgmpeusjYJtUmcSGEI8qYLJZSfmcExwPfScUmlOfiFqhhyebrhIYYYVWFpwA+QgiHCuFnIaVcIKWMllJGBwTY3lDOhnLgwAGmTZvG7Nmz+e+//+jesRuDbgjk6ul9mTr5SgoLC0lPT6d9y1DeDHuPp+fWvt8iLi6uzK/T8uXLWblyJQcOnN2sVDqp0dvJU/mwEsN4+NkX6DnwbIcKjbGmeEVuv/12ArMcKPTI5aeffsLHx6es1tCYpKWlkZKSQlRUFEkJSXz25vtlrtNvvnEi81svpWeCJ9fdcXOjn7ux8PDwwDPPniLXQjp27Mipojh8cwVhkeG0SHchqVUGmyPT2V+0g5iYGAD8nBzZ1j6LoEwXLi25gsxsNQclIfFwmV+2XJds/HOcmzQv3vbKV9eAzrV6JWksTG2GDdfrqTRqHQuBfVLKV80OLQNGAKuFEB0AJ+A0sBz4QgjxKtAaiAQ2AQKIFEK0RRmMq4FpUkophFgNXI7qV7ke+KE+up7LbNy4kYcffpji4uKyh18UZLEhLBkhk9m65gpmTpvB978sI7yvAzECYtJqt3ZIbGwsUVFRhIWF8e2337J7t+rwrmwCYWk7e17GKWgB/bo2bf9BVFQU/rmu5HrmlzVDffvttyxe3LgLWJW+PNu3b889/5vCFxH/4PyBK20jO/JR8K8Miw3i2+f+a3R37Y2Jh4cH7nmOFLgW4ezszCnHk7TKdCGkfwgef7qzM1z1s+U45pZ1yB/I+Qu3QvjrseO0CGrB4q+Uw8dtezfg8Wc4ABmuOfjlWW6YcGVEtxvL5sTNXPbo9TULayxObZxDRgghfhRCJAshkoQQPxgv95oYBFwHjBRC7DC2icBHQIQQYjeGITBqK3uAr4G9wArgdillidFHcgfwG6pj/2tDFuAh4F4hRAyqT2VhnXJ/jrN582YuuOAC/vrrL5599lkcHR0ByCg+DsCzBQ9w1K+A/xy/56/f17Cj01EAEu3iq0qyHA8//DBCCNLS0rj55pvLlletrNmw1NgkFx/E3gQXXtn0HaR+BT6kexWUOaAs9SNWyocfflg2kW/58uW0adOG1NTUOp2j1KC0DW/LOr/NAMzf8AAvLFQ1kuev/cYiNbDGxNPTE5c8J/JciyguKmZfYCoR2W2YNGkSrRzPrB2S45iv8itgU+tD9I8PPpM3F4FzMaSXJPHMM88AkOpegE+xd5Pm5c5H/4+97+Ta/DVvLtRmoPYXqGG7pT2rV6MMQbUeh6WUfwNV9Y1cW0WcecBZ41yllL8AZ81SklIeQY0Ca5YcO3YMUO7GBw4cyL///sv69etJJJZWmXY8/MqLZN2dxnOhHzL0q+6keki88+CER81u3vfu3cvXX3/NY489RnFxMS+88AIODqq4VFZD2b17Nw4ODhx1jaFDsqtVHnA/GcC/nmfGb5ivj3Ls2DFuuukmZs+eXeYROS4ujq+//ppbbrml1ucorQXu+GsDcT7FdD7lyvrwJCCJ8YcjGPCU7Q9d9fDwwPGEE1lBWfyyZBkZrpKefkNxcnKiY+ueqJkCkOVcxLFjx+jdNoptnnu5xTS5LA1fX18cspIo9synlasPAR7+HPU4jO/p869JWVN7alMvd5NSfialLDa2z1H9KhorU1pTKHWFsWDBAlavXs1x90TC09Sop3lvfMATOXfR7VQrbjoygR7bQ4jzLWTlbyurTfvXX5XLlNmzZzNq1ChKSkrKfDdVVkPZtWsX7du351CLNCKyrLNCXgunIHKcINBbvdTMXYOU1qCWLFlCXl4eO3fuBOCzzz6r0zliYmIICgri560LcS6Gr6ZvYEbscO46cRkLn1rXSDmxLB4eHtjnOZLhWsKff38NwISJ6huvbZsznoEzXUo4duwYLVqqRaoG9Btbdszb2xufLGeyfHI5esNRgvrnUmQPfi6Nv/aJ5tyhNgblVyHEw0KIcCFEmDGD/hchhJ8Qws/SCmqqpvTFXroeSGBgIB3ad+CIfx6hxeFlco+99AZrFp1gwSe/0M63G4UO8PJTL1Sb9qpVq+jYsSPBwcEsX/IeQ6Mjy45VVUPp1i6KVDdJO7fGd1deG8IC1HzXkRe3Zfp10ys1KBkZGfzwww/s3LkTDw8PNmzYUNaMVRsOHz5M+/btiXU6TKdTHnQf0JOPF63mjfe/tdlhwhXx8PBA5DmS4wQ78v8mKMOeC0YPBqB3f1XD8s6DTBfJsaPHcPQsAqBLdK+yNLy9vXHLdmZfcC75jrA//BQAgd56ud3mTG0MypXAzcBqYA1qYuLVqNnyWyymmaZGkpOT8fHxKes7+fDl15k2uz/5jtDOu3K33QOj1VdmalJclcN/CwvVutyjR4+muKiYz3y+ZcOEQ7QPDKVNmzZnGZTs7GxiY2PxclET5rpFDm6sLNaJOY8+xaTDnfiq3SYKUhJIT08vO7Z7926CgoIIDQ3lySefJDs7m1mzlJfjf//9t9bniImJoV27dpz0zKJl/rnZvOPh4YEpT92rteGJdDkdUjaIoPfgaB4+eQc9N4QjBaSeTKHYLRfPAsqtze7t7Y1zlivFKhmSPJU7l9atGn8xLc25Q22WAG5bzaZLjxVJTk6mdBj0kb2HmZt0L0vb7QCgR5fKJ5dFD1QLPrn5mlizZs1ZxzMzM3n77bfJzc1l1KhRLP/8G1LdJMX24DEpjfCwcHLSDnJ0/5kJhKWd9fl26it1+PgLGyuLdcLFzYUfPt6Df44g2eMgWVlZlJSUlOnYvXt3brnlFvbv3w/AJZeobsHY2NpNhszJySExMZE2IW044VVCK7vQmiPZIJ6enhTlqe5NKWBwwEXljo+6bDLFWaq/zM4kyXHPIjCrvEcDb29vRPbZXg7atutkIa015wK2O7ZRUyOlBsVUYuL258eS5iqZfXwCEw63Z+KVl1Uap3v/nrgXwsnoOL7+4GwPOnfccQf33nsvfn5+jBgxgt/Xq2G3l8R0Y0ebbPwdBSuG7uKt+Y+WxVm6dCn29vYkOx+nVaYd7btGnpVuU2Fnb0fXpFbEtFKjuUqNyr59++jSpQuzZ8/GxcUFOzs7QluFMmJMGId2H6xV2rt2KceTXvaumOwgxPvcXA1w1KhRtAlpX7Z/50NPljvu7u5OUb4yOO4uzqS7Z9Eit7znam9v7zKj41J0Jrxj9y4W0lpzLqANyjlMUlISAQEB3DhrNCvaHeGaY0N5f+Ev/PLpoSpduNvZ23Ff3q1kuBXzZ/h3ZeGfffYZJ0+e5I8//mDKlCmcOHECHx8fdpr+pd1pJy6OViOhDrZTQ2VP58bz+eef8+STT7Jo0SIuuugi4jxPlQ0GsCZdHPtx3K+Ylp4tyMjI4MiRI+Tn59O1a1datGjBnXfeyejRo3nxxTtZPegYyadrNy+ndDXD4sIsANqHn5uzsyMiIrj4YvXB4Z8j8Ass3xXq4eFBQb5qwnJzdeS0Rz7+Rf7lZLy9vcnLVm7wB8apmppbIbQOOzf6kTSWQRuUc5jk5GR83X34tM1qRh0J5eOFq2sV78lX32HogQHE+5bw929/ERcXx/Tp05k2bRqJiYmMGzcOZ2dnTsadZGfQabpldGTKNdNwKoY9wcoZYErJSR577DGeeOIJkpOTuWHGDRz2zyOsuDZTlCzLiAGXAxAR5ktGRkaZ/7KePXvyxTsLmTB4DD98/wM/eqrVBxNCatfktXXrVgIDAzmVphYs7d6n2pHzNs2oiybR77gfH3b58qxjHh4e5OapaoeLm+CUp4kWduVHb3l7e3M6VZWF8aHX4l4ILXIcbHpCp8by1DgPxZjxfg0QIaV8SgjRBmhluE3RWAmTycTp06dxlnaY7GBo4CV1epi7dhjOt2xg6TcfMmqKWjFx9WplkIYNGwbAK88/SG4gXNr3dnxa+ND5lAc7g5XTwFT7FGJjTzN27FgCAwNJiT1OoQN0aWWdDnlzJk29HK+nr0NEppORkcGyZcsIDg4mODCY4XG98TvgyJRfLyWhVQldTriyq002f/y4kg49OtGmTdWjlLZs2UJ0dDSJOUdwKoYeF/SqUtbWaRXain8XplR6zN3dnex85e/NJbAYKSDQPayczIUXXkhMTAwXdhvLqMnjWXzrmzia9PojzZ3avIHeAS4Aphr7WaiJjhorkp6eTklJCaYS9YJv37byUV1Vccn0q/DKh31ZG8rmZIAaetypk+pY/dW0lA5Jzlxz200AdMxXqzA6lECqizrva6+9xmeffcbv2xdjb4Lps+Y0NGsNxsXNheFHo9jcNZltf29hxYoVTJkyhTde/D8yXeCoXxGvt1pC73hvJsobMdnBM0/O4aabbqoyzdzcXPbu3UufPn04JRIIyXA6bxdw8vDwIDNP3d/8lmolx5DA9uVk2rZty+uvv86YSydiZ2/Ha5O+4+Up356VlqZ5URuD0l9KeTuQDyClTEP539JYiaeeeopJk5QzvHypvjJ79Ktb80uXrl3oeNybQwFx7Ny5k/DwcKKiopgwYQIrliyn2y0e7GmVy4SSi8tqPhP73EhIuj29D/mQ4llAq1at6NxZrUq41WM7PRO8CesQVt1pm4zp4x/DJODXf94hLy+PKVOm8EveEtqmODLmcBht0hz47Jb1jLp0Ev45guKQFP77778q09uxYwcmk4no6GhOuaXSKrtpXYw0Ja6uruQU5mJvgsQAZVDad+hWbZwRF49pVO/RmnOT2hiUImOhLQkghAhAeQjWWIGSkhLeeuutsn6BLJGMSxF07hVVQ8zy2Nvb45sUxJGAIvb/t4+ePXuyceNG3nn7Hf7353Wc8M7l1rhJPP/qp2Vxrr/7Vn64bhOuJ7057S4ZOWwkj865mXZ3OXMgMJ9oaTtuRwaPH06vGG8Ohh3D1dWVAHd/dgRnMSp7BL8sjGHf0xlE9elCu/btaJvgxcngNE6ePFlu7oo5pbW4Du07EOufR+vic3PIcG0oXU3CO19wzF+5r+na52yv0RpNRWpjUOYD3wOBQoh5wN/AsxbVSlMlGzduLOf6JM3pNK0znerVGeptr0bkFKRn0aNHDzw9PZn3v7vZEZzFzKxpvPPhj7i4lfeyExkZiWuJD1LAxaPG877zh2Q5FxOUacd1Vz7UsMw1It7e3rgle3LCp4iozlGs/UO5gusfNQEHR4eyxZjatGmDxwlfjgQUMahne66/tXKjuGvXLry9vdm1YTN5jtC91dAmy4u18DQmP3Y56WYzNU+NbVObiY2LgQeB54BEYIqU8htLK6apnGXLluHo6EjPnj0BOO2eQWC2Z73S6t1TTX708XOkR48eXDK9O8/6fUjnk6488/KHlcbx9PRkzPCLAXhrwwOcdpe81vZ9TrxSwqBxtvOSdXFxgXQXChwgqm1Hdh9dD8DoSZPLyTk5OTGyz+VIAZsuiuHnDnsrXYBs165ddOvWjfVbfgRg3PgrLZ8JK/Luu++S66waIi4osP5AC825QW3c17cDYqWUbwO7gTFCCB9LK9bcWbx4MStWrDgr/JdffmHEiBHMnTuXTp06ccorj4Ci+rkAmTbrBgBC2vsybtw41gbtZvDRlmx4+sRZNRNzwtuqfpO/w08x8FgA19w2q17ntzQluSoPPm6uxBbtpXWGPeGdzh7WfM0Nao5NkT2U2MGuTTvLHZdSlhmUfblbaJllR/Swc3fIcG245ZZbSPZQBuXqi++1sjaac4XatJMsBUqEEO2B91GrJ35hUa2aOTk5OUyfPp0JEybwwAMPlIVLKTly5AjdunXjyiuv5K8//uK0u6SlY/0c8rVp3wbfXEGxdw6ZKZmkukmi7Hrh08Kn2ngdu53poL2q7V31OndTUCzV2JESkclRr5NEpPlXKhcR1Y7wFEeEmqfH3p3byh2Pi4sjIyODbt26cdA3nk6nWzaL+RYjj4TgWAKjpjTtYmmac5faPBUmY5GrS4G3pJQPANpHtQXZtm0bJpOJgIAAlixZUhaekZFBfn4+QUHq8i//Sk1Ka+3TvtJ0akNwhhunnE6yfb1ykBjsV7M7kY49OmFvgvBUR257+OF6n9vSSBc1rDfNFMcR/wLCTVW7hHm6w1tMXj8QgKPH95Y7VupyxdvJk6N+RXS072kZhW2M3z86RubDedZWQ3MOUdtRXlOB6cBPRpij5VTSbNqk5oxec801xMXFkZmphm6WrjbYMrAl188YxuysOXgWwPgJU6tMqyZa5fmT6JnJ/v3K/Ui7ttUPDwVwcHTgkthe3Ox+j03PxfD09SQwy47tnjsosYMOLftWKXvtHbOJ7KI65ONTD5U7Vur6futmtUbMxGEzLKOwjWFnb1dt06dGU5HaGJSZqImN86SUscbyv3VblUhTJzZt2kRYWBjDhw8HYN++fcAZg7Li5wV82nYt445EsOmKvVwwpv6dpi0JIcG7mCMn1ByMHtG1WwDzm0+38fCz1a+pYm2uvfZaWmW4sr9lPo4lMPnS6tcd79W/Fz55gpMFx8qFJyYm4unpyaaCPwhJt+eiay63pNoazTlLbUZ57QXuB3YJIboC8VJK236TnONs2rSJfv360aWL8ty6d69qgik1KPvd/6PLSTd+/vgQnXp1btC5Qrw6UGIHuwv/xaUIovpYZ3EsSzBjxgxaFqjFx0YfbU/3AT2rle/cuTOBGU6kOJRf7yU1NZVW/q3YEpxE/5RuzaL/RKOpD7UZ5TUcOIRyt/IOcFAIYTvjQ88zkpKSOHr0KP369aNt27Y4OjqWGZQTJ04AkOyZQ1BOQKO82Lp1Vs08m4NP1Hs+iy3T2i4cgDsnvlyjbMeOHfHOcCXFI6tceEpKCuEBfuQ7wqjO9W9e1GjOd2rz9ngFGCulHCalHAqMA16zrFrNl82blXv4fv36MWlmB7pf7VrWKZyYmIiriyuJXsUE0KpRznfVTTMITrdXa7HXcz6LLfPI/R/wvusrTLh6co2yrq6uuGe5c8qrsFx4amoqTu5qCG333gMsoqdGcz5QG4PiKKU8ULojpTyI7pS3GJs3b8bOzo7wkHD+anOEre0yKUpRNZPExEQiQ9pTZA8t3cMb5XwOjg4MS1X9JvWdz2LLtO8ayewHaz+Pwq3QlwxXydOPPkVKivKTlpqainBRRqZd1Lm5qJZG0xTUxqBsEUJ8KIQYbmwfoNeStxibNm2iS5cuLFv8KXmO4JMn2DdkDyt+XUFiYiJB/moBq9CAxnuxXTtRDf0NcrL+WibWxtsxGIAvP/q4bMh2amoqxc55uBRBYOtAa6qn0dg0tTEotwJ7gbuMba8RpmlkpJRs2rSJvn378tfBb3AtgmkpF5PobeLx+/+P48eP42asxNq+Q93c1VfHhKsu5h2nF/jfQ+82WprnKj16q+7BgDZ2nD59GpPJRGpqKgXOufjl2p93fUwaTWNSm1FeBVLKV6WUlxrba1LKgpriCSFChRCrhRB7hRB7hBB3G+FPCCEShBA7jG2iWZy5QogYIcQBIcQ4s/DxRliMEOJhs/C2Qoh/jfAlQohz2q1+bGwsKSkp9OvXj21+++idEMiA7hMAkMVZxMbGIl3UKnk9+lU9p6I+3Dr3Qe0AELj7f/cSkmZPYZt0EhISyMzMxGQykeeci0/eOV28NBqLU6VBEULsEkL8V9VWi7SLgfuklFHAAOB2IUSpj/XXpJQ9je0X43xRwNVAF2A88I4Qwt5wnf82MAGIAqaapfOCkVZ7IA24sc5XwIb4/fffAWjpFcBRvyK62/djxIUXAhAQ4QxAnnM6ngUQ2u78dZ9uTVxcXIhKbcOR0HTi4+NJTU0FIMs1H68CNytrp9HYNtVNc57UkISllIko78RIKbOEEPuA4GqiTAa+Mmo/sUKIGKB0ll2MlPIIgBDiK2Cykd5IYJoh8wnwBHDOtdvs3r2bkydP8tRTT9G3b18O79sO9tCv2wRCIkIISbenICCDyZMnk+q+kVaZzrrpxYJ087iAlZ6xZCemlRmUDNcCgvNaWlkzjca2qe6t5AiESCmPmW9ACLVYi94cIUQ40Av41wi6w6jpfCSE8DXCgoE4s2jxRlhV4f5AuuFnzDz8nGPmzJmMGTOGEydO8Prrr7Pz+F/Ym2Di5WpGdni6P8e9krjuois45pdCQK6XlTU+vxk/+loAMrvvYs8WNWQ7zb0EL3yri6bRNHuqMyivA5mVhGcax2qFEMID5bF4jpQyE1WDaAf0RNVgXqltWvVFCDFbCLFFCLHFfHEqW+HEiRNER0fzwQcfMHDgQA7Z7aX9aRcCg9WIojamdsS0KODy+Gs57ltMF/TqeZZk9KUTmLZtFHvaZPPNny/i7OBEjhN4O1TurVij0SiqMygtpZS7KgYaYeG1SVwI4YgyJoullN8Z8U9JKUuklCbgA840ayWgXOOXEmKEVRWeAvgIIRwqhJ+FlHKBlDJaShkdEGBbcy2klCQnJzN69GhmzZqFqcTEgYBU2mee6SCPanUBUsDwI8Hsu3gvCz46e50UTeMy7MIraX/ShVNuJ/Fz9wHA17VxJpNqNOcr1RkUn2qOudaUsFALUy8E9kkpXzULN3d9fwlq0S6A5cDVQghnwwFlJLAJ2AxEGiO6nFAd98ullBJYDZR66rse+KEmvWyNjIwMioqKCAxUtZEv3/+INDdJZ68zThofeuYF3nJ8jl/fjWmw7y5N7WjdujU+qe6c8snC2011xgf4nJMtqhpNk1GdQdkihLipYqAQYhawtRZpDwKuA0ZWGCL8YukIMmAEcA+AlHIP8DVqnssK4HajJlMM3AH8BuwDvjZkAR4C7jU68P1RBuycIikpCYCAgADyc/OZt/suWmbZcec9z5TJODg6cPv/HtauxJuQ1q1b45TqToJ3CX5+6rq3DKzfQmYaTXOhus71OcD3QohrOGNAogEnVM2iWqSUfwOikkO/VBNnHjCvkvBfKotnjPyqnb91G6W0T8fH24fLb+7BvvZ5PJN7L23a65eXNQkODqYoxQmTHbi2Vm5XQsIirKyVRmPbVGlQpJSngIFCiBFAqU/zn6WUfzaJZs2E0hrKki9f4ueOB7nuyGDmfvSSlbXSBAQEkJWmHEKmtlZGv10n7cdLo6mOGof/SilXo/oqNBagtIay31WtcfLpJ+usrJEGwM7OjnEXT2EXr7I3NAvPAmgTqT0JaDTVoWfHWQGTycTJkyeBMzWUwy0yaJujm7lsiRfeeAmvfChwgInx0XoyqUZTA/oJaQJK3aCX8tFHHxEREUFqairJycm0axVOmpukvXsPK2moqQw7eztC091wKoaHb3nf2upoNDaPNigWZuPGjQQGBpatugjw119/kZeXx3///UdSUhJtWqsZ2H26jLSWmpoquMpzNnelXkfPgXoyqUZTE3VyoaKpO7t378ZkMrFr1y6iopRPy61bt5YdS05OxrFFLkLC6MkXW1NVTSU8+qJenFSjqS26hmJh4uPjATh27BgA2dnZ7N+/H4Bdu3aRlJREln8K4amOtArVM7E1Gs25i66hWJiEBOUNptSg7NixAykljo6O7Nq1i+TkZDJbpBORoT3ZajSacxttUCyMeQ3l/vvv588/1TSeiy66iN9++42SghIKfYsZltnOmmpqNBpNg9FNXham1KAcPHiQ+fPns337doKDgxk7diw5OTmEBQRhsoMI/8Zb0lej0Wisga6hWJhSg3Lo0CEAXnjhBZzySnht5xz8HhR0ivHkANCj+2AraqnRaDQNRxsUC5KdnU16ejotWrSgfUtf9h85xdSrpzLx2c6keRSS6Sz5vdN/CAmDx46ytroajUbTIHSTlwUp7ZAf3L0fG684RM8RXiz9+CN2B+UwI/86eif4kusEIRkOtAhqYWVtNRqNpmFog2JBSg2Km1c+ALu6JPD+qecJTXfg8efeoZ/9cABCM/TSshqN5txHGxQLUtp/ctL9IEJCirtkf8t8ZjnchpunG1ddchcAwcXah5dGozn30X0oFqR07snelicYejiI/S1P0SrTjUfeVbOvB08Yzv9WzGLSdddbU02NRqNpFLRBsSD//vsv/aN686/XNq7JG8Fz424lIKhlOa+18974wIoaajQaTeOhm7wshMlkYv369bQJ9QFgcP+LuWDMYNp3jbSuYhqNRmMhtEGxELt37yY9PR07jzwA+g0bYmWNNBqNxrJog2Ih1q1TKy9mO6finyNoHd7ayhppNBqNZdEGxUKsW7eO4OBgTrskE5TpZm11NBqNxuJog2IBpJSsW7eOIUOGcMoji5Z5ftZWSaPRaCyONigWIDY2lhMnTjBwwEASvItoSbC1VdJoNBqLow2KBSjtP2nl1YIiewhy167pNRrN+Y/FDIoQIlQIsVoIsVcIsUcIcXeF4/cJIaQQooWxL4QQ84UQMUKI/4QQvc1krxdCHDK2683C+wghdhlx5gshhKXyUxfWrVuHr68vGSmnAGgXql3TazSa8x9L1lCKgfuklFHAAOB2IUQUKGMDjAWOm8lPACKNbTbwriHrBzwO9Af6AY8LIUqdX70L3GQWb7wF81Nr1q1bx6BBgzhyfDcAXbpHW1kjjUajsTwWMyhSykQp5TbjfxawD8o6E14DHgSkWZTJwKdSsRHwEUIEAeOA36WUqVLKNOB3YLxxzEtKuVFKKYFPgSmWyk9tOXXqFAcPHmTIkCH8m7ESlyKIHjrA2mppNBqNxWkS1ytCiHCgF/CvEGIykCCl3FmhhSoYiDPbjzfCqguPryS8yfn+++957bXX6NevH3369AEgLzGJPyPiuD52GG6eetiwRqM5/7G4QRFCeABLgTmoZrD/oZq7mgwhxGxUMxpt2jS+Z9/XXnuNbdu2sW7dOkJDQ3F1dWVFxiKCsefVF79r9PNpNBqNLWLRUV5CCEeUMVkspfwOaAe0BXYKIY4CIcA2IUQrIAEINYseYoRVFx5SSfhZSCkXSCmjpZTRAQEBjZG1MlJTU1m/fj333HMPXbp0IS4ujr59+3KoRSo9T7fDL1DPQdFoNM0DS47yEsBCYJ+U8lUAKeUuKWWglDJcShmOaqbqLaU8CSwHphujvQYAGVLKROA3YKwQwtfojB8L/GYcyxRCDDDONR34wVL5qYoVK1ZgMpm46KKLuOOOOwDo3akHKe6Sdm56dJdGo2k+WLLJaxBwHbBLCLHDCPuflPKXKuR/ASYCMUAuMBNASpkqhHga2GzIPSWlTDX+3wYsAlyBX42tSfnxxx9p2bIl0dHRdO3alU2bNuHrri5rr6gRTa2ORqPRWA2hBkg1H6Kjo+WWLVsaJa20tDRat27NjBkzePfdd8vCZ80cw8LwP4i7Lo6QiJBqUtBoNJpzAyHEVilltXMg9Ez5BvD555+Tn5/P7Nmzy4UfMe0jLNVBGxONRtOs0Cs2NoAPPviA6OhoevXqxa9f/cBjf8zgipa3sDcgkc7JQdZWT6PRaJoUXUOpJxkZGezatYtLL70UgA9+fpQtoek85PQ8Wc4mrup8dw0paDQazfmFrqHUk5iYGAA6duyIqcTExoC99DvuR8eSLkwdfx8Trp5sZQ01Go2madEGpZ6UGpTIyEi+/ehzEr1LmFlyKfPe+MDKmmk0Go110Aalnhw6dAiAd19/hO99fsLOA2be9KCVtdJoNBrroQ1KPTl06BBBQUF85/sTXvmO3ONyD+27RlpbLY1Go7EaulO+nsTExNC1bWdOeZqYWHwxD857ztoqaTQajVXRBqWeHDp0CB9PNSl0cN+LrayNRqPRWB9tUOpBRkYGycnJZHsk4FoEE6+81NoqaTQajdXRBqUe7NixA4B4vwQ6n/LW651oNBoN2qDUi88//xwfDx/2t8qhQ0FHa6uj0Wg0NoE2KHUkLy+Pr7/+mnGDh1BkD71CR1pbJY1Go7EJtEGpI7/88guZmZk4++YCMHHyVCtrpNFoNLaBNih15NixYwDEOewnNN2Brv30IloajUYDemJjrcnJyaG4uJjs7GwA9rc4SZeUYCtrpdFoNLaDNii1pE+fPvTs2ZPQ0FDa+Adz3DuBy7OrXWtGo9FomhW6yauWeHl5kZmZSXZ2Nq38fQCIDO1tXaU0Go3GhtAGpZZ4eXmRkZFBVlYWbm6OAAQEtLayVhqNRmM7aINSS8xrKE6uAoCWrfUSvxqNRlOKNii1xNvbm8zMTLKysrB3KQEgKFQbFI1GoylFG5RaYl5DwakYgJCIUCtrpdFoNLaDNii1pNSgZGZmUuJUiFsheHh7WFstjUajsRm0QaklXl5ehPkF4S3tKHYqwCtfXzqNRqMxx2JvRSFEqBBitRBirxBijxDibiP8aSHEf0KIHUKIlUKI1ka4EELMF0LEGMd7m6V1vRDikLFdbxbeRwixy4gzXwghLJUfLy8v3CenEjtpPwXO+XgWOFrqVBqNRnNOYsnP7GLgPillFDAAuF0IEQW8JKXsLqXsCfwEPGbITwAijW028C6AEMIPeBzoD/QDHhdC+Bpx3gVuMos33lKZSYqJY3doHskeJnJc8nAvdLLUqTQajeacxGIGRUqZKKXcZvzPAvYBwVLKTDMxd0Aa/ycDn0rFRsBHCBEEjAN+l1KmSinTgN+B8cYxLynlRimlBD4FplgqP1uOL1P5EhDvn4N7kaulTqXRaDTnJE3SESCECAd6Af8a+/OEEHHANZypoQQDcWbR4o2w6sLjKwlvdIqLitkWtB//HNWidtpd4l6iF9XSaDQacyxuUIQQHsBSYE5p7URK+X9SylBgMXBHE+gwWwixRQixJTk5uc7xHRwd+GT8SiKWtysLc8OzMVXUaDSacx6LGhQhhCPKmCyWUn5Xichi4DLjfwJgPrEjxAirLjykkvCzkFIukFJGSymjAwIC6pMVIjpFEBufWrbvKbzrlY5Go9Gcr1hylJcAFgL7pJSvmoVHmolNBvYb/5cD043RXgOADCllIvAbMFYI4Wt0xo8FfjOOZQohBhjnmg78YKn8eHl5cTo3FQc1SR4PRx9LnUqj0WjOSSzpvn4QcB2wSwixwwj7H3CjEKIjYAKOAbcYx34BJgIxQC4wE0BKmSqEeBrYbMg9JaUsrSrcBiwCXIFfjc0ieHp6goTAbHtOeJfg6eJvqVNpNBrNOYnFDIqU8m+gsnkhv1QhL4Hbqzj2EfBRJeFbgK4NULPWODo64ubmhm+W5IR3Ht4e9Ws602g0mvMVPd27Dnh5eeGR5QKAn0+glbXRaDQa20IblDrg5eWFk2FQ9FooGo1GUx69BHAd8PLywi6rEICWrfV68hqNRmOOrqHUAS8vL5KP2DHoaAA9B+r15DUajcYcXUOpA97e3mzLSWXPx2nWVkWj0WhsDl1DqQOtWrWiZcuW1lZDo9FobBJtUOrA008/zc8//2xtNTQajcYm0U1edcDf3x9/fz2hUaPRaCpD11A0Go1G0yhog6LRaDSaRkEbFI1Go9E0CtqgaDQajaZR0AZFo9FoNI2CNigajUajaRS0QdFoNBpNoyDUMiTNByFEMmphr/rQAjjdjOSb4hxa3vrn0PLWP4etyVdGmJSy+oWgpJR6q+UGbGlO8raoU3OTt0Wdmpu8LerUFHmuz6abvDQajUbTKGiDotFoNJpGQRuUurGgmck3xTm0vPXPoeWtfw5bk68Xza5TXqPRaDSWQddQNBqNRtMoaIOi0Wg0mkZBGxQbQAghrK2DxrawxTJhizpZkqbI7/l2TbVBsSC1LSyyGXdk1fWBsqS8MLBE+nVN1xbLhC3qZCmEEPZNkd+mOEd9ynV90QalERFC2Ash3IUQHaDmwiKECBZCLBBCuFtIH28hRC8hhKdZWKMVLCGErxBibB1f2g5CCBfza1Rd/Hpc07qm7yyEaC+EGCANapB3F0KMFkKMrWX6ojZyZvIthBDThRB2ZmHVpe8khGgthOhdU9pmcepULoz0vxRCtK1Jtj7UtRwZeQ4QQvSopbyjcV271lK+DbBLCDGwNvL1QQjhJ4S4WAjRyiysyvexUaY7CiH6CyE8hRBONaTvKoToJ4QYWctyXedyVBl6CeDG5WXAF4gUQqwDngQKpJSmKuRfAOKllDlGYXKRUuYKIRyklMUVhYUQ44F44ICUsqgW+rwFdAVeE0LsAHZLKU3G11dJJen3AWKklBm1SBtgPrBJSrmykrTsqsj3fKAA6CeEKAbukVJuM+KISgxGXa9pXdN/HygGpgghXpFSPlcqU0UeFgIlhvyLwDOVXUszrhFCpABrpZQ51ehRyivArtLzmstWoc8ioAjoL4TYZ+i3TkqZUc09qFO5AJ4GegJjgfdLX05V5aEe5bSu5WgBkGXkOQt4G/hDSplZhV5vAgIYIIR4TUq5qIb0/4e6prOEECellEequZb1yS/A88AI4CMhxC5D/3whhJOUsrAS+Y+BDNR9SwVWCSF+l1LurSLPHxh5mCqEeBN4qJpnBupXjs6mKabjN4fNKBwbgY5AF+AjoHs18mGoh6h0/zHgd+NGTqlEfjRgAl4FLkf51alJp7HALkOXtw0dbwOGVSI7xkj/NSAacK4h7WHAP2b744FHgecwhqNXEmewcY1cjf0VQDLwUiNd07qmP7w0D0AE6iH8H+ph961EfgCw0fjvBCxGPegfAL0qkR+FMlavAPcB/Wq4psNL0zf2rzDSnl+FPr2BnWb7twM/G/LejVQuhgAbDJm9KANaXR7qVE7rWo7M74Gx/wlwCHipsjJbeo+BACPfLwHXoZ63ytIfZuS3BfCGkb5dY+XXLF5X4E/gCeM8s4DHgSsqkY0G/jXbfw7YivogDaqhXDsDXxtl4jWgZ2OVo8o23eTVeNwAzJdSHkA9eEdQhQWASqqSJ4FkIcRAIcQlwCBUodoGPF5JdXsYqiDtBi5BfT2NEUIEGOkPNn6F2e8q4DvUQ7EKVVDmAyHmzR0GFxrpp6JqBfcaTUEORnp9KslvntEEdCXwIJAGdAM2CyFaVHKNugDrpZR5xv6zwGdAeyHEC5XI1/Wa1jX9K1Ffr6BeDBejDFJrYLcQIqyCfDtgvxDCA5iOMmCvoZzuLaukCaYP8CLqQ6ElcJUQYrYwmuOEEBMqyF8FeAshwoQQlwM3A78BfsAOoynGHDdUGWoLIKV829DLA/UF24rKqUu5eAR4WUq5GrgaiBJCXG3oLyppRqlTOaXu5Sgc2CeEcDT2XwHWou7ZJ0II5wryM4A3pJTJQHuUMTEBfVHXtKKzw9nAa1LK06jyEwgsEEIEGnpXfGfWNb8IIexR5flv4ADwOdAD+D+gXSXlrgWQKIQIMvYXooyoA/CNEMKngvyFqA8FjPwOM+LkAT8KIbpUkK9vOTqbulgfvVX6pSEAF9TXZA/OTBZtBfxt/L8c+LWSuFNQXzavA9eahT8CzDHbtwcigUBjPxLVDPEZ6oF8CthXUS/jdxrwufH/B2Ad8Ckw00zWERholn4vI+1fgEmoF9saM3kHVI3mYdQXdDrQx+z4AqB3JfntiHpBTkF9aa4CLgJcUV/L/mayTsY17WoWVu01rWP69kA3s/05QA+z/deBkRXS9zPyuxjYCVxtduw54PoK8v4YNQvUC+9GI92HjWt03EzWDvXCuwVYhmp+GGB2fD4wopJr+ghwP9AWcKogP6yy8mr8XlNduUCVa1/g7tJ7bvxOR33xV5a2HdABCKhNOUWVuzGoWuHC2pQj1Av+c+NaXgv8AVxqHHsPCK3wXA43/tsb98j8Hr8HRJvtuwLjSvNi/IYDH1a8t/V9LivcgwuAb4z/36Cet5eBeyvIO6GaKeeiam+/Y5Q9VO0mqoJ8a7P/Uyvk+Xng8kp0egJlzGtVjqrarP5CPh83wNH4fQf1YlsJjDI7bmcUcm+jkCxDPdCXoF5CO0sLtnnhreQ8A40bng2MrVhgzfYfRBmFXcb+EKBTLfIxCfgJKKyoj3E8BPWVf3eF8M3AoCrSvARYY+j9pFn4v5i94M3CnSvsV3pN65u+2XGvCvv/AsMrkYtANVdchmqiCEC9GDdXdg8q3jegE3AP6uU5pgpdegEzKoRtMteHMy/4bihD8LFxv/oC7qgv2NGVpG1v9v+hepaLO1Bf12dd/yrkKy2nZsddjft2X3XlqFR3YALqA2Ee5T+8tgB9K6QhzP4HV3JNqyqn5vEmAomoZjK7CnK1fi6rOM/dqI+CPcZ+FBWazVDviwjUh8jDmBk3I89n5cGs/DlWCN9Y8b6hmsVaoWqsH6M+wqotR1VtulO+gQghpgKhqIt/Qkr5vjzTMbcV1R77iZRylSHvLKUsMI5nCCGeRzW3jEB9TV0HfCel/M2Qn476uvUSQnwqpdxfem4p5QajQ7CsQ9M8fbNO1mOoL+v7jHjrzPQfh/oqyQWypJTfm6X/kxCiH+ql/lvFvEsp44GvRfnRQg8CsVLK9WZhw1G1h1RU2+wyI8184/jjQIaUclcll7jEkCnteNxUyTWtU/pCCBezY0IqMs30fQg4JqVcY+xfgmruckDVinYKIVJRL8EXUUZlT2X3ANW8Utb5K6XcL4SYBvwnpfy9QvqOqM7ZzUKIA2b6PIyqzaypmL6UcpcQYgaqZjMVZfzDDD3/MEvjEtRLyUkIsU5K+TdwHDUo4X4jrYrlIgLIATKllMtKj0kp3xJChKJqIxXvQRqwTUoZI4zBJVWU09Lnxg04LKX8zPymVyxHFfL8K/BrBfkngRQp5Waz9EMADyFEgpRygZQyoZr0y8qEcQ5p9v8XIcQ1qJpA6f2s03NpxAkCTsvyHff/oWoZLxhx95rJz0A9m61RtcnnK+T5GSO90jwEGdegsFR/83MJIeaiBgGV3rPbjbTbo2ooM1Hvn6tR5TaYCuWoRmprefRW6ddFR2A/6ovzBmApqjngIuO4D7ADaGPsd0Z96dxUSVpuqFqLJ2dqOJFG/NtQTQJXoJohupjFmwSE1CL9ALP/pV8vnVBtv6+iqtKfAF8AQ4zj9qg25VCz/B4ErjNPizPNA0EowxVmdrwDql9oAarp4KkKevmi2vBDqjmHHWe+TgNQD2GbeqZf6TUyuybhqHZ58/S3o0aXPWEcczDL7zRUp6Z3bdI3/vfBaJaoJP1XKV+LCEQ1nYRWl75ZXluj2txdKtyD0nM8jmpWKc1vi7qWC0PG3yzP5vfgA+CxCnrZo756ze+x+XOzDPWRMKlUJ5Shq/a5MSsT7sClZte0Yvrfob7MS9Nvg2qWCqvpnpldE9fSa0odn0uzcyQD96IMvp3ZMb9Kzld6z25GPYMfYjYwA/VxMxCjU76G9AXKuN5vdg/aG+lPMcrErSgD39E47o1q4nUxvyY1vhMb8kJt7huqE+1d47+TcZOvRY0uml5aeM3kP0S1+a4G/qJCm7h5wTJ+P+dMG/ZNwB7UA7vUOHfZi7Yu6ZvtPw88W3rMKGR3oJpQBhvh5i+mL1FfpHtQzRF9KqTnALhVCPsGuNWs0G+hfNOQPeVfttWew5AJaED6NV0jFwyDYewvMUs/FDUy5yaz484V7llN6ftW2K8s/Vlmxx0r6FOr9CvkubJzzDY77lmHcjHEXLaaezDG7LhTBfnqnpvrjHD/OuS5YnNldelfW0kZqutzU6fn0pCbj6pVLQSWowZgBJReG3N9zM5xp/HfB/VcPGV23KcW6ftz5mOv4jX6BKOvBtUvloDq3/sH1axcOlKy0hGbVW1WfymfyxtqVNHnGJ1yRpg9arTQB6U3xQh3RXUk+hv7d6LaJxcDXqi200Vm8v6o5pTSr+GVqK9hH1Tz2CLMXgS1SL8zqpnIXP8RqCGFXmZhAagvmfcob6w8jYel9IU1F9W8scTYb43xEjKL09JIv4NZ2IPA28b/3sBtdTzHcw1Iv6Zr1MX8GqFeph9iNlQZ9aCWdmb3B56u4z2uS/r96ph+F/MyVMtzRJufo5blwvzrt6Z70Mv8HtTiuVlA3Z+biuXaZp5LQ8YJVWMp7byfasT7GOgOjER9RJV+lHiiOvfNB40MBJYZ/wcAH9Yh/dEV0vdC1WRK8/AFcKXxPxRlbFrV651Yn0h6K7uRDqhmgRPAXRWO/UuFzjLUl4v5Q+qOGrZ6ADVpaXYF+bKOXSp04KI68S9oYPr2xgP2DzCtkvS7VyLvbbbvgWo6SEFNNCuXviHjTXnDFwr8aPz/BbPRbfU5Rz3Sr+s1cqigjx/G6DLU6KjrbDn9ep6jruWirvfA0s+NTT2XpXEAd7N9V9QH02+ovr/Knk2fCjr9bFzrpRgtIA1IX5jlvX8leajVgIuz8lmfSHo7q7AMRn0R7Ee1Rb6E2eQr8xtotm/ejPE0qiOzSvkKxy4DVjVW+kZ6Xxt5mAo8AGyvIX1Hs//zgM2V6FkxjpNRgN9HvciWNeQcjZB+Tdeo4oie0r6tV1CDA36w5fQbeo56lotq74GlnxtLp19RvsKxs57LKuTMz/E4ZpMKq9MJNcrsKIaxbqz0Kxy7HDUwpNo8VBm/vhH1VmbdzV96Y1DtjzOpMDbcPE6FQmKPav8cXEv5CGAfVQ93rGv6pV8qoajOxV+MF0ePGtK3N0v/K2BoddfJ+C19md2BMbmsMc7RgPTreg9K059UMX3Kv4hrTL8K+erSt6ur/lXEqfIcjVQuarwHWPi5sXT6VchX+1xWKM9OZno+UlWcSuR7ojwvRFso/QjU4IiBVeWhpq1RX7DNeUN1aNV6RARqhnW48T+4lvJhxv9hxm/FLxn76tKvQf5hzu5IrChvV0G+bFRNHeL4oUa0XW8uj9lXVXXnqIV8ufQtcA9KR1td2lTpV7ymtUm/hjjV5qGmclGJfK3usZmM+Rf0Wc9NDfK1KdeOdUzfsQHpn/Vc1uI+f2lWnr1rKd/B+D+uCp3sq0u/FvKRxv9KjVVtt3pH1Fu5l9vNVFO1r0L++4bKo0YAidIXhFl4VfIuxtahQvr3AN/WQX5OZfL1iUP5oZCCM1+4dzeSfCsgwmy/9Kvs1iquUV3lI1DuPdyM/dIv19saSb4Tqg+pdLhn6RdxlWWornFQnbThnBkuXlO5qKt86RdwxXJ6G5U8NzXIV6a/G6qvrUeF8NurSL86+bqmX+k9qCSN0mt0PbC8ofKoZ8yDs/uzGkW+vpue2FgHhBAXSCn/Kd2Xxh1Bjft+3JAp89hag/wT9ZQ390T8PGp8eboQYo2U8kvDT1Cl6aOq8A6oyW3zpZSbjPBQVPtyRe+rVcm3MZOv6KG2rnGWGD6qbpZS/gUUGf7DwjAmezVQ/lPUyJsjxjUtDfdFtZHXR978HnwEfCWlzDX2HVHejj2AZypJvzby5um/iDJC44CFUspiwy9UVfe4tnHMz7EQNYHRz5jn+bcRHoyas1KxXNRG3lynlw3fWyVCCD/gFSnlVuOaPllH+acqkV+Ial7zMvxO/Z9UEwo96iFfWZmojXw5D+FCiItRw95PSgPjHvihPBRUfPZrI29+jvmo0WAOQojtqD6x4qrSr6V8pV7O60RjWabzfeOMU7nvqKIduYnlr0KNm++D+sr4kWo8naI62/5Ezch+CtXBN96I62HI2NVXvp7ncEEZmf9QI3L+RL1gx3Lmq7oh8pcCq832x6DmDdyGMSKG8n0ZdZUfiHJGWbp/F2p01BecGcLZEPkpqLkFA1Cz2p+j5uabOsVB9Y/8iap1zEQ14dyLMkp+jSB/FWpuR1vUcN4/UKOnnuZM7a8h8hejRqO5c2YIcIJxTb2aWt7Yvwb1LC9CuW0pNwekkmezrvJXoYYBt0W5y/kKM593DZVvyGaHprYMQTVV/IvyLDtfmHkqFUKMaGL5W4B5UsqtUspPgFhUwSmV71VB/iZD/jDqy+RylHO6aOAF4+vE1AD5OseRytXFi6i5ByHAetS8kxWorykaIm9co3+M63EP6sXXCTW3YJ7x1S0bIJ+B8tJqL4S4C+XldSHK+/AqIURwA+WfBF6QUm40ZFsa1xNDR1FBvj5xIoFPpXI90xpVDk+hXp6fCSF8GyjfFjWPI1ZKuQf1MnsfZZDGwVmLptVV3gHl9iYHKJRSLpRSBqOGmc8TZ6+8aGl5UE5iZ6EM44PA00KIaCGEi3EPJjdQfirqHsdK5S5nH+qjB0N+WAPl648lrNT5tqFGfAzljJuDYNQXygHUpKbbUc0YFpdH9Rs4o9rDu3Pmy3wshkdg1Iv91Qrylxj77sD3lHeD8TVnOuXqJF/fOEa4g7F9gDE7HPUltQvlk2pKfeQNfZxQbfofoZphDnGmg78T8C3Qrj7yZvrYoZodS41PL7Njb3H2HIVayRv6uAM3GvuOxnYPyi/b5ErKaJ3jGHITUEspvIKa5xNldq0/5ew5G3WVn4SqTU5HTQrciDIM440y4dFAeT9Up/LtFcI9UR4CejaxvEA5Dy2tcQaiPoJWo1yoPA383gB5B9SHWlfO9LN0xVhbyYjzcX3lG/yubKyEmuOGmsm8EVVdrXF0hCXlUc1Bi1G1gU1U4j7eTLZVhf3tmM10bqh8XeOgOsLfQL1ktxphY4DODZVH9d08gWr3Ng/fRgUDV0/59qgmoGyUIbJHvQh3VXzZ1Ee+kvjTUB8btW6yqCkOqiluAmqI6Wiz8O2VxamH/DhU/8BSynuA3lCxnNRFnjMfU31RvrW2o9YVskcZggOUX/7AovIV8lBxxGQUygAVUbk7oYbKv4OaEf8PlS/2Vif5+m66U74GhBATUS48DqJu7m/S6FCVUm4SQqxBeUvd0oTyfVCFuUxequVDt6Gqza/LM8velqZ/ANUM9buU8qRZ/l5GTfY6WB/5BpyjJ6oWUILyPbQB9ZV+s5H33yukX1f5PsY1TUa5FikxO/4yagW8Qw2Q7wXEAAlSypFCiMtQ/UZeqFFBK6WUOxogX3o9i4xjpZ34y1GOFoeiHDhWdg9qjGOW50OoGscaI+8fCiESUc1w26SUDZU/iPKI2xczhBCvAkdKy0k95Mu8A0vlYbin0Yz4EcrLt5dxDXY3hXzFOGZhdkYae4UQsShP4lsbQV4YYaVNmFtQkzhflFJur498Y1BaBdJUglAuud9EPZAOqGYdO9RQxD+FWrnvJtRImkwryi+XUv5htIX+gXI0l16L9DuhHpCJ9ZGv5TWq6RwuxuX+HUiTZm7vq0i/rvJuqBreD4Y+bQ19LqlCn7rKu6OM6Luol21/1Gp8uVLKokaQL3c9jTw6oL44S5cpqFOcaq7px6jmpjtRNYTfpZR5jSDvjvEhIKVcJZSb9ftRtY/KynVN8hegOrDfkVK+gRlGPvugjHemcU0tKm+EVxrH6GMpEUI4AZOBFVLKrEaUd5BqFF9HlGuWaKOc1kmeRkIblGoQQryCqh28I9Qymx1Ro2faoJzfHRFCOJoVKmvKvymlPGp0iqbVQv4tKWWsEMJLGmuB1FW+kc/RGlggpTxs9hXVmPLm19RDSpndiPIDUZ3JL0q1RkzZl2AjyZfTx1y+tnmoxTkuQA0rflFKmYgZjSRfsUyYr0lTV/k/UC5IvFBLCDwvpfzZ7PxehuEpza9F5WupU1kZsoS8IeMu1eCBOss3GrKR2s7Oxw01RHId0N4sLATleO4LznbVbk35rzi7w7I6+S8xcyZXH/lGPsdrRp4rDnFtLPlGyXMt5OuqT13kz8pvfeLUohy5WlD+S+pWrsvJozrDH0ENTXdCDV3eiOpv8UM1+z3UVPK1jNMLeNiC8n2A/9VXvjE3q7+0bX1DTTb7AjVPxNksfDWVzPtobvK2qFNzk7dFnZpA3nzd85aoicU7UM2VlzW1vC3qVJ88NHTTTV5VIIyZwUIIf9SQxQtQM3VLZ+yOklIObK7ytqhTc5O3RZ2aSr7CNShtqnoTNRJvfFPJ26JO9clDo2EJK3U+bqihg5NQ8yvuwmwlRi1vmzo1N3lb1MlS8pzp/y1dMdMZNSClKm/CFpW3RZ3qk4eGbhZJ9FzfzG6EuVfRe6nEw21zlLdFnZqbvC3q1ITy5t6HH+DMaottmlLeFnWqTx4ac9PzUMwQaqheiTTmIMgzo6ueR7myLm7O8raoU3OTt0WdrCBfbIQ/h/LGkGKEH28KeVvUqT55sATaoJTnUaCTEGIPypXHL1JNNisyjlVsn2xu8raoU3OTt0WdrCVfbCV5W9SpPnlofKQFqz/n0oZqr/0Z5fb7etRoiMXADZitgNZc5W1Rp+Ymb4s6NTd5W9SpPnmw1KZrKKgREKgx2+9KNQnsiBAiAjWGOxrl5mO/NKx7c5O3RZ2am7wt6tTc5G1Rp/rkwaI0hdU6FzaUZf8HNcnKH+VKog/K0+/C5i5vizo1N3lb1Km5yduiTvXJg6W2JjuRrW+o/qRZqKrjjxgzSYEAYC1nz5huVvK2qFNzk7dFnZqbvC3qVJ88WGqz+ovc2htmbiNQLgucAG/OrPX9BcoPTrOUt0Wdmpu8LerU3ORtUaf65MHSm9Vf6NbcgOGoJWVHYuamwDgmUM7pvjO7Qc1K3hZ1am7ytqhTc5O3RZ3qk4em2Jq16xUhxHHU+hA7gURgrTTWpDCTMfco2qzkbVGn5iZvizo1N3lb1Kk+eWgKmq1BEUKEohZnWoxa07mPcWgf8AlwMZAnpVzRHOVtUafmJm+LOjU3eVvUqT55aCqas0GxQ7l7zzT226GWNQ0H8lEL+kyQUq5ujvK2qFNzk7dFnZqbvC3qVJ88NBmyCdvXbGVDrWYXgJkPIbNjwahq5EfNVd4WdWpu8raoU3OTt0Wd6pOHptysclJrbkB31PC6BcC/wP0Vjruh1soOao7ytqhTc5O3RZ2am7wt6lSfPDT1ZvUXfJNnGH4D7gZaAYOATai2x5FmMu2aq7wt6tTc5G1Rp+Ymb4s61ScPTb1Z/QXfpJlVC/b8APSrEH49sAYY3JzlbVGn5iZvizo1N3lb1Kk+ebDGZkczQkqZhropM4UQLmbhnwBfA8Oas7wt6tTc5G1Rp+Ymb4s61ScP1qDZGBQhRIQQYhjKE2cAcEwIcYeZiD1nht81O3lb1Km5yduiTs1N3hZ1qk8erEWzGDYshAhCWXEJnADeBdKBj4FMYBcwCpgmpdzR3OT1NbK+vL4H1pc/H+4B1sbabW5NsaEufqnDtIuBGMDH2B8G9KN851ezkrdFnZqbvC3q1NzkbVGn+uTBmpvVFbB4BtXY7NVAqFnYW8Bjxn9/YGxzlbdFnZqbvC3q1NzkbVGn+uTB2pvVFWiSTEJX1MzS0v0+wGLj/w/A7OYsb4s6NTd5W9Spucnbok71yYM1t+bSh2LuVM0RcEVNDopBDcMb25zlbVGn5iZvizo1N3lb1Kk+ebAq1rZo1tqAVwETMFzLnxs6NTd5W9Spucnbok71yUNTbQ5Vm5rzngUoj5xrtPw5o1Nzk7dFnZqbvC3qVJ88NAnNosmrKoQQdlJKk5Y/d3RqbvK2qFNzk7dFneqTh6agWRsUjUaj0TQezWamvEaj0WgsizYoGo1Go2kUtEHRaDQaTaOgDYpGY0GEECVCiB1CiD1CiJ1CiPuEWsK1ujjhQohpTaWjRtNYaIOi0ViWPCllTyllF2AMau3vx2uIEw5og6I559CjvDQaCyKEyJZSepjtRwCbgRZAGPAZ4G4cvkNKuUEIsRHoDMQCnwDzgeeB4YAz8LaU8v0my4RGU0u0QdFoLEhFg2KEpQMdgSzAJKXMF0JEAl9KKaOFEMNR64VPMuRnA4FSymeEEM7AeuAKKWVsE2ZFo6mR5jxTXqOxNo7AW0KInkAJ0KEKubFAdyHE5ca+NxCJqsFoNDaDNigaTRNiNHmVAEmovpRTQA9Uf2Z+VdGAO6WUvzWJkhpNPdGd8hpNEyGECADeA96Sqq3ZG0g0XGhch1rKFVRTmKdZ1N+AWw1vswghOggh3NFobAxdQ9FoLIurEGIHqnmrGNUJ/6px7B1gqRBiOrACyDHC/wNKhBA7gUXAG6iRX9uEEAJIBqY0jfoaTe3RnfIajUajaRR0k5dGo9FoGgVtUDQajUbTKGiDotFoNJpGQRsUjUaj0TQK2qBoNBqNplHQBkWj0Wg0jYI2KBqNRqNpFLRB0Wg0Gk2j8P9FyXM7fgRvSgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(data_test.index, y_test, c='k')\n",
        "plt.plot(data_test.index, predictions, c='b')\n",
        "plt.plot(data_test.index, predictions, c='r')\n",
        "plt.plot(data_test.index, predictions, c='g')\n",
        "plt.xticks (range(0, 252, 10), rotation=60)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close price')\n",
        "plt.legend(['Truth', 'Neural network prediction' ])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}